{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec06fe58",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<!-- <a href=\"https://colab.research.google.com/github/i1idan/schizophrenia-diagnosis-eeg-signals/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pfZzZofyZVB_",
   "metadata": {
    "id": "pfZzZofyZVB_"
   },
   "source": [
    "# Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3uUKOYfZUiA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3uUKOYfZUiA",
    "outputId": "1e927faf-42ef-421a-f8e8-7d640bfb6b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deep_utils>=0.8.20 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.8.20)\n",
      "Requirement already satisfied: tensorflow>=2.8.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.8.0)\n",
      "Collecting keras_unet_collection\n",
      "  Downloading keras_unet_collection-0.1.13-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 51 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.27.1 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from deep_utils>=0.8.20->-r requirements.txt (line 1)) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from deep_utils>=0.8.20->-r requirements.txt (line 1)) (4.62.3)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.58 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from deep_utils>=0.8.20->-r requirements.txt (line 1)) (4.5.5.62)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from deep_utils>=0.8.20->-r requirements.txt (line 1)) (1.21.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (2.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (1.43.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (3.19.4)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (1.13.3)\n",
      "Requirement already satisfied: setuptools in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (58.0.4)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (13.0.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (2.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (4.1.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorflow>=2.8.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->-r requirements.txt (line 2)) (0.37.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests>=2.27.1->deep_utils>=0.8.20->-r requirements.txt (line 1)) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests>=2.27.1->deep_utils>=0.8.20->-r requirements.txt (line 1)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests>=2.27.1->deep_utils>=0.8.20->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests>=2.27.1->deep_utils>=0.8.20->-r requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (2.6.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.8.0->-r requirements.txt (line 2)) (3.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: keras-unet-collection\n",
      "Successfully installed keras-unet-collection-0.1.13\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lCirzbPuZyuQ",
   "metadata": {
    "id": "lCirzbPuZyuQ"
   },
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35dc016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016\n",
    "dataset_name = \"ISIC_2016\"\n",
    "train_zip_url = \"https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part1_Training_Data.zip\"\n",
    "test_zip_url = \"https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part1_Test_Data.zip\"\n",
    "mask_train_zip_url = \"https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part1_Training_GroundTruth.zip\"\n",
    "mask_test_zip_url = \"https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part1_Test_GroundTruth.zip\"\n",
    "train_path = \"data/train\"\n",
    "test_path = \"data/test\"\n",
    "mask_train_path = \"data/mask_train\"\n",
    "mask_test_path = \"data/mask_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94ccc6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_name = train_zip_url.split(\"/\")[-1]\n",
    "\n",
    "import os\n",
    "os.path.isfile(zip_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "166bc7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/deep_utils/vision/face_detection/ultralight/__init__.py:3: UserWarning: \n",
      "No module named 'deep_utils.vision.face_detection.ultralight.torch'. If you don't use UltralightTorchFaceDetector ignore this message.\n",
      "  UltralightTorchFaceDetector = import_module(\n",
      "/home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/deep_utils/vision/face_detection/ultralight/__init__.py:6: UserWarning: \n",
      "No module named 'deep_utils.vision.face_detection.ultralight.tf'. If you don't use UltralightTFFaceDetector ignore this message.\n",
      "  UltralightTFFaceDetector = import_module(\n",
      "[INFO] zip file: ISBI2016_ISIC_Part1_Training_Data doesn't exist! Unzipping ISBI2016_ISIC_Part1_Training_Data.zip\n",
      "[INFO] Successfully extracted ISBI2016_ISIC_Part1_Training_Data.zip to .!\n",
      "[INFO] zip file: ISBI2016_ISIC_Part1_Training_GroundTruth doesn't exist! Unzipping ISBI2016_ISIC_Part1_Training_GroundTruth.zip\n",
      "[INFO] Successfully extracted ISBI2016_ISIC_Part1_Training_GroundTruth.zip to .!\n",
      "[INFO] zip file: ISBI2016_ISIC_Part1_Test_Data doesn't exist! Unzipping ISBI2016_ISIC_Part1_Test_Data.zip\n",
      "[INFO] Successfully extracted ISBI2016_ISIC_Part1_Test_Data.zip to .!\n",
      "[INFO] zip file: ISBI2016_ISIC_Part1_Test_GroundTruth doesn't exist! Unzipping ISBI2016_ISIC_Part1_Test_GroundTruth.zip\n",
      "[INFO] Successfully extracted ISBI2016_ISIC_Part1_Test_GroundTruth.zip to .!\n",
      "[INFO] Preprocessing is successfully done!\n"
     ]
    }
   ],
   "source": [
    "!python data/preprocessing.py \\\n",
    "--dataset_name $dataset_name \\\n",
    "--train_zip_url $train_zip_url \\\n",
    "--test_zip_url $test_zip_url \\\n",
    "--mask_train_zip_url $mask_train_zip_url \\\n",
    "--mask_test_zip_url $mask_test_zip_url \\\n",
    "--train_path $train_path \\\n",
    "--test_path $test_path \\\n",
    "--mask_train_path $mask_train_path \\\n",
    "--mask_test_path $mask_test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130c324",
   "metadata": {
    "id": "4130c324"
   },
   "source": [
    "# Training Models\n",
    "## UnetRes50\n",
    "### Main params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a3fd2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic\n",
    "dataset_name = \"ISIC_2016\"\n",
    "model = 'unet_res50'\n",
    "save_path = \"saved_models/\"\n",
    "train_identifier = \"usual\"\n",
    "multi_train = 3\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "img_size = (256, 256)\n",
    "img_channel = 3\n",
    "loss = \"dice_loss\"\n",
    "\n",
    "# optimizer\n",
    "optimizer = 'adam'\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "min_lr = 1e-5\n",
    "\n",
    "# callbacks\n",
    "reduce_lr_patience = 10\n",
    "reduce_lr_factor = 0.1\n",
    "early_stopping_p = 25\n",
    "\n",
    "seeds = 1234\n",
    "verbose = 1\n",
    "\n",
    "# Augmentation\n",
    "hair_aug_p = 0\n",
    "hair_rmv_p = 0\n",
    "random_rotate_p = 0.5\n",
    "p_horizontal_flip = 0.5\n",
    "p_vertical_flip = 0.5\n",
    "p_center_crop = 0.5\n",
    "mosaic_p = 0\n",
    "cutmix_p = 0\n",
    "cutmix_beta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c877bc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/deep_utils/vision/face_detection/ultralight/__init__.py:3: UserWarning: \n",
      "No module named 'deep_utils.vision.face_detection.ultralight.torch'. If you don't use UltralightTorchFaceDetector ignore this message.\n",
      "  UltralightTorchFaceDetector = import_module(\n",
      "/home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/deep_utils/vision/face_detection/ultralight/__init__.py:6: UserWarning: \n",
      "No module named 'deep_utils.vision.face_detection.ultralight.tf'. If you don't use UltralightTFFaceDetector ignore this message.\n",
      "  UltralightTFFaceDetector = import_module(\n",
      "2022-02-26 22:02:16.313099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:02:16.330304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:02:16.330439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "[INFO] Successfully created logger for SKIN-CANCER\n",
      "[INFO] Saving params!\n",
      "2022-02-26 22:02:16,331 - SKIN-CANCER - INFO - [INFO] Params are successfully saved in saved_models/ISIC_2016/unet_res50/usual_0/exp_2/params.txt!\n",
      "2022-02-26 22:02:16,331 - SKIN-CANCER - INFO - Chosen Model: unet_res50\n",
      "2022-02-26 22:02:16,331 - SKIN-CANCER - INFO -  Arguments: Namespace(optimizer='adam', batch_size=8, img_channel=3, transfer_learning_epochs=5, finetuning_epochs=10, epochs=50, lr=0.001, min_lr=1e-05, reduce_lr_patience=5, reduce_lr_factor=0.1, early_stopping_p=20, mlflow_source='./mlruns', cutmix_p=0.0, cutmix_beta=0.0, usual_aug_with_cutmix=False, hair_aug_p=0.0, hair_rmv_p=0.0, random_rotate_p=0.5, p_horizontal_flip=0.5, p_vertical_flip=0.5, p_center_crop=0.5, mosaic_p=0.0, loss='dice_loss', label_smoothing=0, focal_loss_gamma=2, pos_weight=1, neg_weight=1, train_path='data/train', test_path='data/test', mask_train_path='data/mask_train', mask_test_path='data/mask_test', val_path='data/val', mask_val_path='data/mask_val', verbose=1, save_path='saved_models/', dataset_name='ISIC_2016', seed=1234, img_size=(256, 256), save_path_name='usual_0', model='unet_res50')\n",
      "[INFO] Successfully created MLFLOW-Handler\n",
      "2022-02-26 22:02:16,386 - SKIN-CANCER - INFO - Data Loader is successfully loaded!\n",
      "2022-02-26 22:02:16.395035: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-26 22:02:16.395251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:02:16.395382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:02:16.395446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:02:16.651735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:02:16.651872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:02:16.651958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:02:16.652012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21805 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Model: \"ResNet50_U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 262, 262, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 128, 128, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 128, 128, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 128, 128, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 130, 130, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 64, 64, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 64, 64, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 64, 64, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 64, 64, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 32, 32, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 32, 32, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 32, 32, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 16, 16, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 16, 16, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 16, 16, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 16, 16, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 16, 16, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 32, 32, 512)  2097664    ['conv4_block6_out[0][0]']       \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 1024  0           ['conv2d_transpose[0][0]',       \n",
      "                                )                                 'conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 512)  4719104     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 512)  2048       ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 512)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 512)  2359808     ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 256)  524544     ['activation_1[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 512)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 256)  1179904     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 64, 64, 256)  590080      ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 12  131200     ['activation_3[0][0]']           \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                2)                                'conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 128, 128, 12  221312      ['concatenate_2[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 128, 128, 12  512        ['conv2d_4[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_4[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 128, 128, 12  147584      ['activation_4[0][0]']           \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128, 128, 12  512        ['conv2d_5[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_5[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 64  32832      ['activation_5[0][0]']           \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256, 256, 67  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                )                                 'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 256, 256, 64  38656       ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 256, 256, 64  256        ['conv2d_6[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 256, 256, 64  0           ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 256, 256, 64  36928       ['activation_6[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 256, 256, 64  256        ['conv2d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 256, 256, 64  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 256, 256, 1)  65          ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,676,545\n",
      "Trainable params: 20,642,113\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "2022-02-26 22:02:17,297 - SKIN-CANCER - INFO - model: unet_res50 is successfully loaded!\n",
      "2022-02-26 22:02:17.298041: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-02-26 22:02:17.298053: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-02-26 22:02:17.298069: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1630] Profiler found 1 GPUs\n",
      "2022-02-26 22:02:17.298207: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.6/lib64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-26 22:02:17.356286: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-02-26 22:02:17.356444: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "[INFO] Successfully created following callbacks [model_checkpoint, reduce_lr, early_stop, csv_logger, tensorboard]\n",
      "2022-02-26 22:02:17,359 - SKIN-CANCER - INFO - loss: dice_loss is successfully created!\n",
      "2022-02-26 22:02:17,363 - SKIN-CANCER - INFO - Start train for saved_models/ISIC_2016/unet_res50/usual_0/exp_2\n",
      "Epoch 1/50\n",
      "2022-02-26 22:02:20.300097: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8301\n",
      " 1/90 [..............................] - ETA: 6:59 - loss: 0.6835 - dice_score: 0.3165 - iou: 0.1880 - dice_loss: 0.6835 - jaccard_loss: 0.7197 - focal_tversky_loss: 0.68972022-02-26 22:02:22.323203: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-02-26 22:02:22.323223: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-02-26 22:02:22.948457: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-02-26 22:02:22.948878: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-02-26 22:02:22.965128: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 1686 callback api events and 1694 activity events. \n",
      "2022-02-26 22:02:22.978232: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-02-26 22:02:22.994644: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22\n",
      "\n",
      "2022-02-26 22:02:23.008377: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22/ai.trace.json.gz\n",
      "2022-02-26 22:02:23.034032: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22\n",
      "\n",
      "2022-02-26 22:02:23.037353: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22/ai.memory_profile.json.gz\n",
      "2022-02-26 22:02:23.038500: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22\n",
      "Dumped tool data for xplane.pb to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22/ai.xplane.pb\n",
      "Dumped tool data for overview_page.pb to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22/ai.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22/ai.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22/ai.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_02_22/ai.kernel_stats.pb\n",
      "\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2254 - dice_score: 0.7746 - iou: 0.6416 - dice_loss: 0.2254 - jaccard_loss: 0.2964 - focal_tversky_loss: 0.3008\n",
      "Epoch 00001: val_loss improved from inf to 0.99301, saving model to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/model\n",
      "2022-02-26 22:02:52.924823: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "90/90 [==============================] - 39s 387ms/step - loss: 0.2254 - dice_score: 0.7746 - iou: 0.6416 - dice_loss: 0.2254 - jaccard_loss: 0.2964 - focal_tversky_loss: 0.3008 - val_loss: 0.9930 - val_dice_score: 0.0070 - val_iou: 0.0035 - val_dice_loss: 0.9930 - val_jaccard_loss: 0.9964 - val_focal_tversky_loss: 0.9962 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1506 - dice_score: 0.8494 - iou: 0.7411 - dice_loss: 0.1506 - jaccard_loss: 0.2401 - focal_tversky_loss: 0.2372\n",
      "Epoch 00002: val_loss improved from 0.99301 to 0.98594, saving model to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/model\n",
      "90/90 [==============================] - 34s 376ms/step - loss: 0.1506 - dice_score: 0.8494 - iou: 0.7411 - dice_loss: 0.1506 - jaccard_loss: 0.2401 - focal_tversky_loss: 0.2372 - val_loss: 0.9859 - val_dice_score: 0.0141 - val_iou: 0.0071 - val_dice_loss: 0.9859 - val_jaccard_loss: 0.9927 - val_focal_tversky_loss: 0.9923 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1438 - dice_score: 0.8562 - iou: 0.7532 - dice_loss: 0.1438 - jaccard_loss: 0.2360 - focal_tversky_loss: 0.2308\n",
      "Epoch 00003: val_loss improved from 0.98594 to 0.48851, saving model to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/model\n",
      "90/90 [==============================] - 34s 378ms/step - loss: 0.1438 - dice_score: 0.8562 - iou: 0.7532 - dice_loss: 0.1438 - jaccard_loss: 0.2360 - focal_tversky_loss: 0.2308 - val_loss: 0.4885 - val_dice_score: 0.5115 - val_iou: 0.3537 - val_dice_loss: 0.4885 - val_jaccard_loss: 0.6429 - val_focal_tversky_loss: 0.4884 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1258 - dice_score: 0.8742 - iou: 0.7789 - dice_loss: 0.1258 - jaccard_loss: 0.2124 - focal_tversky_loss: 0.2135\n",
      "Epoch 00004: val_loss did not improve from 0.48851\n",
      "90/90 [==============================] - 28s 310ms/step - loss: 0.1258 - dice_score: 0.8742 - iou: 0.7789 - dice_loss: 0.1258 - jaccard_loss: 0.2124 - focal_tversky_loss: 0.2135 - val_loss: 0.9970 - val_dice_score: 0.0030 - val_iou: 0.0015 - val_dice_loss: 0.9970 - val_jaccard_loss: 0.9985 - val_focal_tversky_loss: 0.9984 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1254 - dice_score: 0.8746 - iou: 0.7812 - dice_loss: 0.1254 - jaccard_loss: 0.2124 - focal_tversky_loss: 0.2107\n",
      "Epoch 00005: val_loss did not improve from 0.48851\n",
      "90/90 [==============================] - 28s 310ms/step - loss: 0.1254 - dice_score: 0.8746 - iou: 0.7812 - dice_loss: 0.1254 - jaccard_loss: 0.2124 - focal_tversky_loss: 0.2107 - val_loss: 0.9995 - val_dice_score: 4.6117e-04 - val_iou: 2.2666e-04 - val_dice_loss: 0.9995 - val_jaccard_loss: 0.9998 - val_focal_tversky_loss: 0.9997 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1249 - dice_score: 0.8751 - iou: 0.7818 - dice_loss: 0.1249 - jaccard_loss: 0.2123 - focal_tversky_loss: 0.2119\n",
      "Epoch 00006: val_loss did not improve from 0.48851\n",
      "90/90 [==============================] - 28s 305ms/step - loss: 0.1249 - dice_score: 0.8751 - iou: 0.7818 - dice_loss: 0.1249 - jaccard_loss: 0.2123 - focal_tversky_loss: 0.2119 - val_loss: 0.9981 - val_dice_score: 0.0019 - val_iou: 9.5336e-04 - val_dice_loss: 0.9981 - val_jaccard_loss: 0.9990 - val_focal_tversky_loss: 0.9990 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1194 - dice_score: 0.8806 - iou: 0.7894 - dice_loss: 0.1194 - jaccard_loss: 0.2052 - focal_tversky_loss: 0.2050\n",
      "Epoch 00007: val_loss did not improve from 0.48851\n",
      "90/90 [==============================] - 28s 310ms/step - loss: 0.1194 - dice_score: 0.8806 - iou: 0.7894 - dice_loss: 0.1194 - jaccard_loss: 0.2052 - focal_tversky_loss: 0.2050 - val_loss: 0.7111 - val_dice_score: 0.2889 - val_iou: 0.1732 - val_dice_loss: 0.7111 - val_jaccard_loss: 0.8260 - val_focal_tversky_loss: 0.8214 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1147 - dice_score: 0.8853 - iou: 0.7967 - dice_loss: 0.1147 - jaccard_loss: 0.1987 - focal_tversky_loss: 0.1984\n",
      "Epoch 00008: val_loss did not improve from 0.48851\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "90/90 [==============================] - 28s 315ms/step - loss: 0.1147 - dice_score: 0.8853 - iou: 0.7967 - dice_loss: 0.1147 - jaccard_loss: 0.1987 - focal_tversky_loss: 0.1984 - val_loss: 0.5801 - val_dice_score: 0.4199 - val_iou: 0.2687 - val_dice_loss: 0.5801 - val_jaccard_loss: 0.7284 - val_focal_tversky_loss: 0.7191 - lr: 0.0010\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.1086 - dice_score: 0.8914 - iou: 0.8062 - dice_loss: 0.1086 - jaccard_loss: 0.1891 - focal_tversky_loss: 0.1968\n",
      "Epoch 00009: val_loss did not improve from 0.48851\n",
      "90/90 [==============================] - 28s 308ms/step - loss: 0.1086 - dice_score: 0.8914 - iou: 0.8062 - dice_loss: 0.1086 - jaccard_loss: 0.1891 - focal_tversky_loss: 0.1968 - val_loss: 0.5561 - val_dice_score: 0.4439 - val_iou: 0.2976 - val_dice_loss: 0.5561 - val_jaccard_loss: 0.6999 - val_focal_tversky_loss: 0.7005 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1017 - dice_score: 0.8983 - iou: 0.8175 - dice_loss: 0.1017 - jaccard_loss: 0.1780 - focal_tversky_loss: 0.1813\n",
      "Epoch 00010: val_loss improved from 0.48851 to 0.23403, saving model to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/model\n",
      "90/90 [==============================] - 34s 383ms/step - loss: 0.1017 - dice_score: 0.8983 - iou: 0.8175 - dice_loss: 0.1017 - jaccard_loss: 0.1780 - focal_tversky_loss: 0.1813 - val_loss: 0.2340 - val_dice_score: 0.7660 - val_iou: 0.6309 - val_dice_loss: 0.2340 - val_jaccard_loss: 0.3643 - val_focal_tversky_loss: 0.3796 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0966 - dice_score: 0.9034 - iou: 0.8259 - dice_loss: 0.0966 - jaccard_loss: 0.1693 - focal_tversky_loss: 0.1762\n",
      "Epoch 00011: val_loss improved from 0.23403 to 0.16587, saving model to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/model\n",
      "90/90 [==============================] - 34s 382ms/step - loss: 0.0966 - dice_score: 0.9034 - iou: 0.8259 - dice_loss: 0.0966 - jaccard_loss: 0.1693 - focal_tversky_loss: 0.1762 - val_loss: 0.1659 - val_dice_score: 0.8341 - val_iou: 0.7253 - val_dice_loss: 0.1659 - val_jaccard_loss: 0.2695 - val_focal_tversky_loss: 0.2855 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0883 - dice_score: 0.9117 - iou: 0.8388 - dice_loss: 0.0883 - jaccard_loss: 0.1563 - focal_tversky_loss: 0.1623\n",
      "Epoch 00012: val_loss improved from 0.16587 to 0.11103, saving model to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/model\n",
      "90/90 [==============================] - 33s 372ms/step - loss: 0.0883 - dice_score: 0.9117 - iou: 0.8388 - dice_loss: 0.0883 - jaccard_loss: 0.1563 - focal_tversky_loss: 0.1623 - val_loss: 0.1110 - val_dice_score: 0.8890 - val_iou: 0.8021 - val_dice_loss: 0.1110 - val_jaccard_loss: 0.1923 - val_focal_tversky_loss: 0.1984 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0822 - dice_score: 0.9178 - iou: 0.8492 - dice_loss: 0.0822 - jaccard_loss: 0.1458 - focal_tversky_loss: 0.1541\n",
      "Epoch 00013: val_loss improved from 0.11103 to 0.10215, saving model to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/model\n",
      "90/90 [==============================] - 34s 378ms/step - loss: 0.0822 - dice_score: 0.9178 - iou: 0.8492 - dice_loss: 0.0822 - jaccard_loss: 0.1458 - focal_tversky_loss: 0.1541 - val_loss: 0.1022 - val_dice_score: 0.8978 - val_iou: 0.8173 - val_dice_loss: 0.1022 - val_jaccard_loss: 0.1777 - val_focal_tversky_loss: 0.1781 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0813 - dice_score: 0.9187 - iou: 0.8508 - dice_loss: 0.0813 - jaccard_loss: 0.1444 - focal_tversky_loss: 0.1548\n",
      "Epoch 00014: val_loss did not improve from 0.10215\n",
      "90/90 [==============================] - 28s 307ms/step - loss: 0.0813 - dice_score: 0.9187 - iou: 0.8508 - dice_loss: 0.0813 - jaccard_loss: 0.1444 - focal_tversky_loss: 0.1548 - val_loss: 0.1159 - val_dice_score: 0.8841 - val_iou: 0.7953 - val_dice_loss: 0.1159 - val_jaccard_loss: 0.1995 - val_focal_tversky_loss: 0.1875 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0813 - dice_score: 0.9187 - iou: 0.8504 - dice_loss: 0.0813 - jaccard_loss: 0.1447 - focal_tversky_loss: 0.1542\n",
      "Epoch 00015: val_loss did not improve from 0.10215\n",
      "90/90 [==============================] - 28s 309ms/step - loss: 0.0813 - dice_score: 0.9187 - iou: 0.8504 - dice_loss: 0.0813 - jaccard_loss: 0.1447 - focal_tversky_loss: 0.1542 - val_loss: 0.1208 - val_dice_score: 0.8792 - val_iou: 0.7943 - val_dice_loss: 0.1208 - val_jaccard_loss: 0.2009 - val_focal_tversky_loss: 0.1879 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0768 - dice_score: 0.9232 - iou: 0.8580 - dice_loss: 0.0768 - jaccard_loss: 0.1374 - focal_tversky_loss: 0.1462\n",
      "Epoch 00016: val_loss did not improve from 0.10215\n",
      "90/90 [==============================] - 28s 305ms/step - loss: 0.0768 - dice_score: 0.9232 - iou: 0.8580 - dice_loss: 0.0768 - jaccard_loss: 0.1374 - focal_tversky_loss: 0.1462 - val_loss: 0.1126 - val_dice_score: 0.8874 - val_iou: 0.8026 - val_dice_loss: 0.1126 - val_jaccard_loss: 0.1927 - val_focal_tversky_loss: 0.1876 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0798 - dice_score: 0.9202 - iou: 0.8536 - dice_loss: 0.0798 - jaccard_loss: 0.1417 - focal_tversky_loss: 0.1491\n",
      "Epoch 00017: val_loss improved from 0.10215 to 0.10146, saving model to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/model\n",
      "90/90 [==============================] - 34s 381ms/step - loss: 0.0798 - dice_score: 0.9202 - iou: 0.8536 - dice_loss: 0.0798 - jaccard_loss: 0.1417 - focal_tversky_loss: 0.1491 - val_loss: 0.1015 - val_dice_score: 0.8985 - val_iou: 0.8188 - val_dice_loss: 0.1015 - val_jaccard_loss: 0.1766 - val_focal_tversky_loss: 0.1736 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0770 - dice_score: 0.9230 - iou: 0.8579 - dice_loss: 0.0770 - jaccard_loss: 0.1374 - focal_tversky_loss: 0.1471\n",
      "Epoch 00018: val_loss did not improve from 0.10146\n",
      "90/90 [==============================] - 28s 311ms/step - loss: 0.0770 - dice_score: 0.9230 - iou: 0.8579 - dice_loss: 0.0770 - jaccard_loss: 0.1374 - focal_tversky_loss: 0.1471 - val_loss: 0.1136 - val_dice_score: 0.8864 - val_iou: 0.8000 - val_dice_loss: 0.1136 - val_jaccard_loss: 0.1954 - val_focal_tversky_loss: 0.1929 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0751 - dice_score: 0.9249 - iou: 0.8609 - dice_loss: 0.0751 - jaccard_loss: 0.1346 - focal_tversky_loss: 0.1442\n",
      "Epoch 00019: val_loss improved from 0.10146 to 0.09623, saving model to saved_models/ISIC_2016/unet_res50/usual_0/exp_2/model\n",
      "90/90 [==============================] - 34s 378ms/step - loss: 0.0751 - dice_score: 0.9249 - iou: 0.8609 - dice_loss: 0.0751 - jaccard_loss: 0.1346 - focal_tversky_loss: 0.1442 - val_loss: 0.0962 - val_dice_score: 0.9038 - val_iou: 0.8267 - val_dice_loss: 0.0962 - val_jaccard_loss: 0.1689 - val_focal_tversky_loss: 0.1714 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0755 - dice_score: 0.9245 - iou: 0.8603 - dice_loss: 0.0755 - jaccard_loss: 0.1353 - focal_tversky_loss: 0.1441\n",
      "Epoch 00020: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0755 - dice_score: 0.9245 - iou: 0.8603 - dice_loss: 0.0755 - jaccard_loss: 0.1353 - focal_tversky_loss: 0.1441 - val_loss: 0.1005 - val_dice_score: 0.8995 - val_iou: 0.8193 - val_dice_loss: 0.1005 - val_jaccard_loss: 0.1763 - val_focal_tversky_loss: 0.1712 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0752 - dice_score: 0.9248 - iou: 0.8610 - dice_loss: 0.0752 - jaccard_loss: 0.1348 - focal_tversky_loss: 0.1436\n",
      "Epoch 00021: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0752 - dice_score: 0.9248 - iou: 0.8610 - dice_loss: 0.0752 - jaccard_loss: 0.1348 - focal_tversky_loss: 0.1436 - val_loss: 0.0973 - val_dice_score: 0.9027 - val_iou: 0.8247 - val_dice_loss: 0.0973 - val_jaccard_loss: 0.1716 - val_focal_tversky_loss: 0.1834 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0738 - dice_score: 0.9262 - iou: 0.8634 - dice_loss: 0.0738 - jaccard_loss: 0.1325 - focal_tversky_loss: 0.1417\n",
      "Epoch 00022: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.0738 - dice_score: 0.9262 - iou: 0.8634 - dice_loss: 0.0738 - jaccard_loss: 0.1325 - focal_tversky_loss: 0.1417 - val_loss: 0.1144 - val_dice_score: 0.8856 - val_iou: 0.7996 - val_dice_loss: 0.1144 - val_jaccard_loss: 0.1965 - val_focal_tversky_loss: 0.1732 - lr: 1.0000e-04\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.0736 - dice_score: 0.9264 - iou: 0.8636 - dice_loss: 0.0736 - jaccard_loss: 0.1322 - focal_tversky_loss: 0.1431\n",
      "Epoch 00023: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0736 - dice_score: 0.9264 - iou: 0.8636 - dice_loss: 0.0736 - jaccard_loss: 0.1322 - focal_tversky_loss: 0.1431 - val_loss: 0.0980 - val_dice_score: 0.9020 - val_iou: 0.8262 - val_dice_loss: 0.0980 - val_jaccard_loss: 0.1698 - val_focal_tversky_loss: 0.1684 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0706 - dice_score: 0.9294 - iou: 0.8689 - dice_loss: 0.0706 - jaccard_loss: 0.1272 - focal_tversky_loss: 0.1369\n",
      "Epoch 00024: val_loss did not improve from 0.09623\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "90/90 [==============================] - 28s 307ms/step - loss: 0.0706 - dice_score: 0.9294 - iou: 0.8689 - dice_loss: 0.0706 - jaccard_loss: 0.1272 - focal_tversky_loss: 0.1369 - val_loss: 0.1126 - val_dice_score: 0.8874 - val_iou: 0.8075 - val_dice_loss: 0.1126 - val_jaccard_loss: 0.1888 - val_focal_tversky_loss: 0.1792 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0696 - dice_score: 0.9304 - iou: 0.8703 - dice_loss: 0.0696 - jaccard_loss: 0.1257 - focal_tversky_loss: 0.1392\n",
      "Epoch 00025: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 28s 307ms/step - loss: 0.0696 - dice_score: 0.9304 - iou: 0.8703 - dice_loss: 0.0696 - jaccard_loss: 0.1257 - focal_tversky_loss: 0.1392 - val_loss: 0.1037 - val_dice_score: 0.8963 - val_iou: 0.8166 - val_dice_loss: 0.1037 - val_jaccard_loss: 0.1798 - val_focal_tversky_loss: 0.1749 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0691 - dice_score: 0.9309 - iou: 0.8717 - dice_loss: 0.0691 - jaccard_loss: 0.1244 - focal_tversky_loss: 0.1343\n",
      "Epoch 00026: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 306ms/step - loss: 0.0691 - dice_score: 0.9309 - iou: 0.8717 - dice_loss: 0.0691 - jaccard_loss: 0.1244 - focal_tversky_loss: 0.1343 - val_loss: 0.1161 - val_dice_score: 0.8839 - val_iou: 0.8073 - val_dice_loss: 0.1161 - val_jaccard_loss: 0.1891 - val_focal_tversky_loss: 0.1866 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0712 - dice_score: 0.9288 - iou: 0.8683 - dice_loss: 0.0712 - jaccard_loss: 0.1278 - focal_tversky_loss: 0.1386\n",
      "Epoch 00027: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0712 - dice_score: 0.9288 - iou: 0.8683 - dice_loss: 0.0712 - jaccard_loss: 0.1278 - focal_tversky_loss: 0.1386 - val_loss: 0.1098 - val_dice_score: 0.8902 - val_iou: 0.8105 - val_dice_loss: 0.1098 - val_jaccard_loss: 0.1859 - val_focal_tversky_loss: 0.1814 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0670 - dice_score: 0.9330 - iou: 0.8749 - dice_loss: 0.0670 - jaccard_loss: 0.1211 - focal_tversky_loss: 0.1326\n",
      "Epoch 00028: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0670 - dice_score: 0.9330 - iou: 0.8749 - dice_loss: 0.0670 - jaccard_loss: 0.1211 - focal_tversky_loss: 0.1326 - val_loss: 0.1021 - val_dice_score: 0.8979 - val_iou: 0.8207 - val_dice_loss: 0.1021 - val_jaccard_loss: 0.1757 - val_focal_tversky_loss: 0.1767 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0668 - dice_score: 0.9332 - iou: 0.8755 - dice_loss: 0.0668 - jaccard_loss: 0.1206 - focal_tversky_loss: 0.1319\n",
      "Epoch 00029: val_loss did not improve from 0.09623\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0668 - dice_score: 0.9332 - iou: 0.8755 - dice_loss: 0.0668 - jaccard_loss: 0.1206 - focal_tversky_loss: 0.1319 - val_loss: 0.1082 - val_dice_score: 0.8918 - val_iou: 0.8121 - val_dice_loss: 0.1082 - val_jaccard_loss: 0.1843 - val_focal_tversky_loss: 0.1782 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0671 - dice_score: 0.9329 - iou: 0.8748 - dice_loss: 0.0671 - jaccard_loss: 0.1211 - focal_tversky_loss: 0.1329\n",
      "Epoch 00030: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 28s 308ms/step - loss: 0.0671 - dice_score: 0.9329 - iou: 0.8748 - dice_loss: 0.0671 - jaccard_loss: 0.1211 - focal_tversky_loss: 0.1329 - val_loss: 0.1081 - val_dice_score: 0.8919 - val_iou: 0.8132 - val_dice_loss: 0.1081 - val_jaccard_loss: 0.1832 - val_focal_tversky_loss: 0.1792 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0680 - dice_score: 0.9320 - iou: 0.8735 - dice_loss: 0.0680 - jaccard_loss: 0.1224 - focal_tversky_loss: 0.1329\n",
      "Epoch 00031: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 306ms/step - loss: 0.0680 - dice_score: 0.9320 - iou: 0.8735 - dice_loss: 0.0680 - jaccard_loss: 0.1224 - focal_tversky_loss: 0.1329 - val_loss: 0.1070 - val_dice_score: 0.8930 - val_iou: 0.8142 - val_dice_loss: 0.1070 - val_jaccard_loss: 0.1823 - val_focal_tversky_loss: 0.1759 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0699 - dice_score: 0.9301 - iou: 0.8701 - dice_loss: 0.0699 - jaccard_loss: 0.1258 - focal_tversky_loss: 0.1370\n",
      "Epoch 00032: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0699 - dice_score: 0.9301 - iou: 0.8701 - dice_loss: 0.0699 - jaccard_loss: 0.1258 - focal_tversky_loss: 0.1370 - val_loss: 0.1115 - val_dice_score: 0.8885 - val_iou: 0.8093 - val_dice_loss: 0.1115 - val_jaccard_loss: 0.1871 - val_focal_tversky_loss: 0.1855 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0686 - dice_score: 0.9314 - iou: 0.8723 - dice_loss: 0.0686 - jaccard_loss: 0.1237 - focal_tversky_loss: 0.1359\n",
      "Epoch 00033: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 28s 308ms/step - loss: 0.0686 - dice_score: 0.9314 - iou: 0.8723 - dice_loss: 0.0686 - jaccard_loss: 0.1237 - focal_tversky_loss: 0.1359 - val_loss: 0.1133 - val_dice_score: 0.8867 - val_iou: 0.8114 - val_dice_loss: 0.1133 - val_jaccard_loss: 0.1851 - val_focal_tversky_loss: 0.1841 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0660 - dice_score: 0.9340 - iou: 0.8768 - dice_loss: 0.0660 - jaccard_loss: 0.1193 - focal_tversky_loss: 0.1303\n",
      "Epoch 00034: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0660 - dice_score: 0.9340 - iou: 0.8768 - dice_loss: 0.0660 - jaccard_loss: 0.1193 - focal_tversky_loss: 0.1303 - val_loss: 0.1125 - val_dice_score: 0.8875 - val_iou: 0.8104 - val_dice_loss: 0.1125 - val_jaccard_loss: 0.1861 - val_focal_tversky_loss: 0.1791 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0677 - dice_score: 0.9323 - iou: 0.8738 - dice_loss: 0.0677 - jaccard_loss: 0.1223 - focal_tversky_loss: 0.1330\n",
      "Epoch 00035: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0677 - dice_score: 0.9323 - iou: 0.8738 - dice_loss: 0.0677 - jaccard_loss: 0.1223 - focal_tversky_loss: 0.1330 - val_loss: 0.1092 - val_dice_score: 0.8908 - val_iou: 0.8119 - val_dice_loss: 0.1092 - val_jaccard_loss: 0.1845 - val_focal_tversky_loss: 0.1785 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0676 - dice_score: 0.9324 - iou: 0.8743 - dice_loss: 0.0676 - jaccard_loss: 0.1218 - focal_tversky_loss: 0.1337\n",
      "Epoch 00036: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0676 - dice_score: 0.9324 - iou: 0.8743 - dice_loss: 0.0676 - jaccard_loss: 0.1218 - focal_tversky_loss: 0.1337 - val_loss: 0.1162 - val_dice_score: 0.8838 - val_iou: 0.8066 - val_dice_loss: 0.1162 - val_jaccard_loss: 0.1899 - val_focal_tversky_loss: 0.1859 - lr: 1.0000e-05\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.0675 - dice_score: 0.9325 - iou: 0.8742 - dice_loss: 0.0675 - jaccard_loss: 0.1217 - focal_tversky_loss: 0.1337\n",
      "Epoch 00037: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 300ms/step - loss: 0.0675 - dice_score: 0.9325 - iou: 0.8742 - dice_loss: 0.0675 - jaccard_loss: 0.1217 - focal_tversky_loss: 0.1337 - val_loss: 0.1154 - val_dice_score: 0.8846 - val_iou: 0.8048 - val_dice_loss: 0.1154 - val_jaccard_loss: 0.1916 - val_focal_tversky_loss: 0.1872 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0656 - dice_score: 0.9344 - iou: 0.8775 - dice_loss: 0.0656 - jaccard_loss: 0.1185 - focal_tversky_loss: 0.1289\n",
      "Epoch 00038: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0656 - dice_score: 0.9344 - iou: 0.8775 - dice_loss: 0.0656 - jaccard_loss: 0.1185 - focal_tversky_loss: 0.1289 - val_loss: 0.1017 - val_dice_score: 0.8983 - val_iou: 0.8230 - val_dice_loss: 0.1017 - val_jaccard_loss: 0.1735 - val_focal_tversky_loss: 0.1707 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0657 - dice_score: 0.9343 - iou: 0.8772 - dice_loss: 0.0657 - jaccard_loss: 0.1187 - focal_tversky_loss: 0.1310\n",
      "Epoch 00039: val_loss did not improve from 0.09623\n",
      "90/90 [==============================] - 28s 305ms/step - loss: 0.0657 - dice_score: 0.9343 - iou: 0.8772 - dice_loss: 0.0657 - jaccard_loss: 0.1187 - focal_tversky_loss: 0.1310 - val_loss: 0.1152 - val_dice_score: 0.8848 - val_iou: 0.8098 - val_dice_loss: 0.1152 - val_jaccard_loss: 0.1867 - val_focal_tversky_loss: 0.1805 - lr: 1.0000e-05\n",
      "2022-02-26 22:21:13,714 - SKIN-CANCER - INFO - Training is done model is saved in saved_models/ISIC_2016/unet_res50/usual_0/exp_2\n",
      "Evaluation\n",
      "48/48 [==============================] - 8s 175ms/step - loss: 0.1033 - dice_score: 0.8967 - iou: 0.8287 - dice_loss: 0.1033 - jaccard_loss: 0.1675 - focal_tversky_loss: 0.1621\n",
      "2022-02-26 22:21:22,138 - SKIN-CANCER - INFO - Test: Loss= 0.1033005639910698, Dice-Score: 0.8966994881629944, IoU: 0.8287282586097717, Dice_loss: 0.1033005639910698, jaccard_loss: 0.16754841804504395, focal_tversky_loss: 0.16214989125728607\n",
      "1/1 [==============================] - 1s 802ms/step - loss: 0.0894 - dice_score: 0.9106 - iou: 0.8358 - dice_loss: 0.0894 - jaccard_loss: 0.1575 - focal_tversky_loss: 0.1160\n",
      "2022-02-26 22:21:26,392 - SKIN-CANCER - INFO - Successfully Saved figures!\n",
      "\n",
      "-----------------------------train 0 is done! ----------------------------------\n",
      "\n",
      "/home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/deep_utils/vision/face_detection/ultralight/__init__.py:3: UserWarning: \n",
      "No module named 'deep_utils.vision.face_detection.ultralight.torch'. If you don't use UltralightTorchFaceDetector ignore this message.\n",
      "  UltralightTorchFaceDetector = import_module(\n",
      "/home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/deep_utils/vision/face_detection/ultralight/__init__.py:6: UserWarning: \n",
      "No module named 'deep_utils.vision.face_detection.ultralight.tf'. If you don't use UltralightTFFaceDetector ignore this message.\n",
      "  UltralightTFFaceDetector = import_module(\n",
      "2022-02-26 22:21:30.410081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:21:30.437705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:21:30.437797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "[INFO] Successfully created logger for SKIN-CANCER\n",
      "[INFO] Saving params!\n",
      "2022-02-26 22:21:30,439 - SKIN-CANCER - INFO - [INFO] Params are successfully saved in saved_models/ISIC_2016/unet_res50/usual_1/exp_2/params.txt!\n",
      "2022-02-26 22:21:30,439 - SKIN-CANCER - INFO - Chosen Model: unet_res50\n",
      "2022-02-26 22:21:30,439 - SKIN-CANCER - INFO -  Arguments: Namespace(optimizer='adam', batch_size=8, img_channel=3, transfer_learning_epochs=5, finetuning_epochs=10, epochs=50, lr=0.001, min_lr=1e-05, reduce_lr_patience=5, reduce_lr_factor=0.1, early_stopping_p=20, mlflow_source='./mlruns', cutmix_p=0.0, cutmix_beta=0.0, usual_aug_with_cutmix=False, hair_aug_p=0.0, hair_rmv_p=0.0, random_rotate_p=0.5, p_horizontal_flip=0.5, p_vertical_flip=0.5, p_center_crop=0.5, mosaic_p=0.0, loss='dice_loss', label_smoothing=0, focal_loss_gamma=2, pos_weight=1, neg_weight=1, train_path='data/train', test_path='data/test', mask_train_path='data/mask_train', mask_test_path='data/mask_test', val_path='data/val', mask_val_path='data/mask_val', verbose=1, save_path='saved_models/', dataset_name='ISIC_2016', seed=1235, img_size=(256, 256), save_path_name='usual_1', model='unet_res50')\n",
      "[INFO] Successfully created MLFLOW-Handler\n",
      "2022-02-26 22:21:30,520 - SKIN-CANCER - INFO - Data Loader is successfully loaded!\n",
      "2022-02-26 22:21:30.529149: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-26 22:21:30.529317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:21:30.529424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:21:30.529476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:21:30.839260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:21:30.839375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:21:30.839435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:21:30.839487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21826 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Model: \"ResNet50_U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 262, 262, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 128, 128, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 128, 128, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 128, 128, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 130, 130, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 64, 64, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 64, 64, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 64, 64, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 64, 64, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 32, 32, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 32, 32, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 32, 32, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 16, 16, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 16, 16, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 16, 16, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 16, 16, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                )                                                                 \r\n",
      "                                                                                                  \r\n",
      " conv4_block6_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block6_3_conv[0][0]']    \r\n",
      " ization)                       )                                                                 \r\n",
      "                                                                                                  \r\n",
      " conv4_block6_add (Add)         (None, 16, 16, 1024  0           ['conv4_block5_out[0][0]',       \r\n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \r\n",
      "                                                                                                  \r\n",
      " conv4_block6_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block6_add[0][0]']       \r\n",
      "                                )                                                                 \r\n",
      "                                                                                                  \r\n",
      " conv2d_transpose (Conv2DTransp  (None, 32, 32, 512)  2097664    ['conv4_block6_out[0][0]']       \r\n",
      " ose)                                                                                             \r\n",
      "                                                                                                  \r\n",
      " concatenate (Concatenate)      (None, 32, 32, 1024  0           ['conv2d_transpose[0][0]',       \r\n",
      "                                )                                 'conv3_block4_out[0][0]']       \r\n",
      "                                                                                                  \r\n",
      " conv2d (Conv2D)                (None, 32, 32, 512)  4719104     ['concatenate[0][0]']            \r\n",
      "                                                                                                  \r\n",
      " batch_normalization (BatchNorm  (None, 32, 32, 512)  2048       ['conv2d[0][0]']                 \r\n",
      " alization)                                                                                       \r\n",
      "                                                                                                  \r\n",
      " activation (Activation)        (None, 32, 32, 512)  0           ['batch_normalization[0][0]']    \r\n",
      "                                                                                                  \r\n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 512)  2359808     ['activation[0][0]']             \r\n",
      "                                                                                                  \r\n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_1[0][0]']               \r\n",
      " rmalization)                                                                                     \r\n",
      "                                                                                                  \r\n",
      " activation_1 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_1[0][0]']  \r\n",
      "                                                                                                  \r\n",
      " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 256)  524544     ['activation_1[0][0]']           \r\n",
      " spose)                                                                                           \r\n",
      "                                                                                                  \r\n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 512)  0           ['conv2d_transpose_1[0][0]',     \r\n",
      "                                                                  'conv2_block3_out[0][0]']       \r\n",
      "                                                                                                  \r\n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 256)  1179904     ['concatenate_1[0][0]']          \r\n",
      "                                                                                                  \r\n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_2[0][0]']               \r\n",
      " rmalization)                                                                                     \r\n",
      "                                                                                                  \r\n",
      " activation_2 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_2[0][0]']  \r\n",
      "                                                                                                  \r\n",
      " conv2d_3 (Conv2D)              (None, 64, 64, 256)  590080      ['activation_2[0][0]']           \r\n",
      "                                                                                                  \r\n",
      " batch_normalization_3 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_3[0][0]']               \r\n",
      " rmalization)                                                                                     \r\n",
      "                                                                                                  \r\n",
      " activation_3 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_3[0][0]']  \r\n",
      "                                                                                                  \r\n",
      " conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 12  131200     ['activation_3[0][0]']           \r\n",
      " spose)                         8)                                                                \r\n",
      "                                                                                                  \r\n",
      " concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['conv2d_transpose_2[0][0]',     \r\n",
      "                                2)                                'conv1_relu[0][0]']             \r\n",
      "                                                                                                  \r\n",
      " conv2d_4 (Conv2D)              (None, 128, 128, 12  221312      ['concatenate_2[0][0]']          \r\n",
      "                                8)                                                                \r\n",
      "                                                                                                  \r\n",
      " batch_normalization_4 (BatchNo  (None, 128, 128, 12  512        ['conv2d_4[0][0]']               \r\n",
      " rmalization)                   8)                                                                \r\n",
      "                                                                                                  \r\n",
      " activation_4 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_4[0][0]']  \r\n",
      "                                8)                                                                \r\n",
      "                                                                                                  \r\n",
      " conv2d_5 (Conv2D)              (None, 128, 128, 12  147584      ['activation_4[0][0]']           \r\n",
      "                                8)                                                                \r\n",
      "                                                                                                  \r\n",
      " batch_normalization_5 (BatchNo  (None, 128, 128, 12  512        ['conv2d_5[0][0]']               \r\n",
      " rmalization)                   8)                                                                \r\n",
      "                                                                                                  \r\n",
      " activation_5 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_5[0][0]']  \r\n",
      "                                8)                                                                \r\n",
      "                                                                                                  \r\n",
      " conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 64  32832      ['activation_5[0][0]']           \r\n",
      " spose)                         )                                                                 \r\n",
      "                                                                                                  \r\n",
      " concatenate_3 (Concatenate)    (None, 256, 256, 67  0           ['conv2d_transpose_3[0][0]',     \r\n",
      "                                )                                 'input_1[0][0]']                \r\n",
      "                                                                                                  \r\n",
      " conv2d_6 (Conv2D)              (None, 256, 256, 64  38656       ['concatenate_3[0][0]']          \r\n",
      "                                )                                                                 \r\n",
      "                                                                                                  \r\n",
      " batch_normalization_6 (BatchNo  (None, 256, 256, 64  256        ['conv2d_6[0][0]']               \r\n",
      " rmalization)                   )                                                                 \r\n",
      "                                                                                                  \r\n",
      " activation_6 (Activation)      (None, 256, 256, 64  0           ['batch_normalization_6[0][0]']  \r\n",
      "                                )                                                                 \r\n",
      "                                                                                                  \r\n",
      " conv2d_7 (Conv2D)              (None, 256, 256, 64  36928       ['activation_6[0][0]']           \r\n",
      "                                )                                                                 \r\n",
      "                                                                                                  \r\n",
      " batch_normalization_7 (BatchNo  (None, 256, 256, 64  256        ['conv2d_7[0][0]']               \r\n",
      " rmalization)                   )                                                                 \r\n",
      "                                                                                                  \r\n",
      " activation_7 (Activation)      (None, 256, 256, 64  0           ['batch_normalization_7[0][0]']  \r\n",
      "                                )                                                                 \r\n",
      "                                                                                                  \r\n",
      " conv2d_8 (Conv2D)              (None, 256, 256, 1)  65          ['activation_7[0][0]']           \r\n",
      "                                                                                                  \r\n",
      "==================================================================================================\r\n",
      "Total params: 20,676,545\r\n",
      "Trainable params: 20,642,113\r\n",
      "Non-trainable params: 34,432\r\n",
      "__________________________________________________________________________________________________\r\n",
      "2022-02-26 22:21:31,565 - SKIN-CANCER - INFO - model: unet_res50 is successfully loaded!\r\n",
      "2022-02-26 22:21:31.566152: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\r\n",
      "2022-02-26 22:21:31.566165: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\r\n",
      "2022-02-26 22:21:31.566811: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1630] Profiler found 1 GPUs\r\n",
      "2022-02-26 22:21:31.566962: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.6/lib64\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-26 22:21:31.624082: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-02-26 22:21:31.624249: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "[INFO] Successfully created following callbacks [model_checkpoint, reduce_lr, early_stop, csv_logger, tensorboard]\n",
      "2022-02-26 22:21:31,627 - SKIN-CANCER - INFO - loss: dice_loss is successfully created!\n",
      "2022-02-26 22:21:31,631 - SKIN-CANCER - INFO - Start train for saved_models/ISIC_2016/unet_res50/usual_1/exp_2\n",
      "Epoch 1/50\n",
      "2022-02-26 22:21:34.446928: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8301\n",
      " 1/90 [..............................] - ETA: 7:18 - loss: 0.5796 - dice_score: 0.4204 - iou: 0.2661 - dice_loss: 0.5796 - jaccard_loss: 0.6187 - focal_tversky_loss: 0.68282022-02-26 22:21:36.764707: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-02-26 22:21:36.764729: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-02-26 22:21:37.353718: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-02-26 22:21:37.354085: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-02-26 22:21:37.388091: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 1686 callback api events and 1694 activity events. \n",
      "2022-02-26 22:21:37.401918: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-02-26 22:21:37.419434: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37\n",
      "\n",
      "2022-02-26 22:21:37.433109: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37/ai.trace.json.gz\n",
      "2022-02-26 22:21:37.459341: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37\n",
      "\n",
      "2022-02-26 22:21:37.462671: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37/ai.memory_profile.json.gz\n",
      "2022-02-26 22:21:37.463778: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37\n",
      "Dumped tool data for xplane.pb to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37/ai.xplane.pb\n",
      "Dumped tool data for overview_page.pb to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37/ai.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37/ai.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37/ai.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_21_37/ai.kernel_stats.pb\n",
      "\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2099 - dice_score: 0.7901 - iou: 0.6629 - dice_loss: 0.2099 - jaccard_loss: 0.2976 - focal_tversky_loss: 0.2946\n",
      "Epoch 00001: val_loss improved from inf to 0.54266, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "2022-02-26 22:22:07.227346: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "90/90 [==============================] - 39s 387ms/step - loss: 0.2099 - dice_score: 0.7901 - iou: 0.6629 - dice_loss: 0.2099 - jaccard_loss: 0.2976 - focal_tversky_loss: 0.2946 - val_loss: 0.5427 - val_dice_score: 0.4573 - val_iou: 0.3006 - val_dice_loss: 0.5427 - val_jaccard_loss: 0.6912 - val_focal_tversky_loss: 0.5250 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1632 - dice_score: 0.8368 - iou: 0.7234 - dice_loss: 0.1632 - jaccard_loss: 0.2645 - focal_tversky_loss: 0.2529\n",
      "Epoch 00002: val_loss did not improve from 0.54266\n",
      "90/90 [==============================] - 28s 308ms/step - loss: 0.1632 - dice_score: 0.8368 - iou: 0.7234 - dice_loss: 0.1632 - jaccard_loss: 0.2645 - focal_tversky_loss: 0.2529 - val_loss: 0.9789 - val_dice_score: 0.0211 - val_iou: 0.0107 - val_dice_loss: 0.9789 - val_jaccard_loss: 0.9891 - val_focal_tversky_loss: 0.9885 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1366 - dice_score: 0.8634 - iou: 0.7627 - dice_loss: 0.1366 - jaccard_loss: 0.2296 - focal_tversky_loss: 0.2243\n",
      "Epoch 00003: val_loss improved from 0.54266 to 0.44155, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 34s 376ms/step - loss: 0.1366 - dice_score: 0.8634 - iou: 0.7627 - dice_loss: 0.1366 - jaccard_loss: 0.2296 - focal_tversky_loss: 0.2243 - val_loss: 0.4416 - val_dice_score: 0.5584 - val_iou: 0.3917 - val_dice_loss: 0.4416 - val_jaccard_loss: 0.6053 - val_focal_tversky_loss: 0.5705 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1254 - dice_score: 0.8746 - iou: 0.7795 - dice_loss: 0.1254 - jaccard_loss: 0.2143 - focal_tversky_loss: 0.2127\n",
      "Epoch 00004: val_loss did not improve from 0.44155\n",
      "90/90 [==============================] - 28s 311ms/step - loss: 0.1254 - dice_score: 0.8746 - iou: 0.7795 - dice_loss: 0.1254 - jaccard_loss: 0.2143 - focal_tversky_loss: 0.2127 - val_loss: 0.8421 - val_dice_score: 0.1579 - val_iou: 0.0863 - val_dice_loss: 0.8421 - val_jaccard_loss: 0.9129 - val_focal_tversky_loss: 0.9094 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1234 - dice_score: 0.8766 - iou: 0.7827 - dice_loss: 0.1234 - jaccard_loss: 0.2120 - focal_tversky_loss: 0.2106\n",
      "Epoch 00005: val_loss did not improve from 0.44155\n",
      "90/90 [==============================] - 28s 312ms/step - loss: 0.1234 - dice_score: 0.8766 - iou: 0.7827 - dice_loss: 0.1234 - jaccard_loss: 0.2120 - focal_tversky_loss: 0.2106 - val_loss: 0.4843 - val_dice_score: 0.5157 - val_iou: 0.3495 - val_dice_loss: 0.4843 - val_jaccard_loss: 0.6490 - val_focal_tversky_loss: 0.6226 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1285 - dice_score: 0.8715 - iou: 0.7761 - dice_loss: 0.1285 - jaccard_loss: 0.2193 - focal_tversky_loss: 0.2156\n",
      "Epoch 00006: val_loss did not improve from 0.44155\n",
      "90/90 [==============================] - 28s 308ms/step - loss: 0.1285 - dice_score: 0.8715 - iou: 0.7761 - dice_loss: 0.1285 - jaccard_loss: 0.2193 - focal_tversky_loss: 0.2156 - val_loss: 0.7671 - val_dice_score: 0.2329 - val_iou: 0.1339 - val_dice_loss: 0.7671 - val_jaccard_loss: 0.8614 - val_focal_tversky_loss: 0.8599 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1322 - dice_score: 0.8678 - iou: 0.7705 - dice_loss: 0.1322 - jaccard_loss: 0.2254 - focal_tversky_loss: 0.2191\n",
      "Epoch 00007: val_loss did not improve from 0.44155\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.1322 - dice_score: 0.8678 - iou: 0.7705 - dice_loss: 0.1322 - jaccard_loss: 0.2254 - focal_tversky_loss: 0.2191 - val_loss: 0.9974 - val_dice_score: 0.0026 - val_iou: 0.0013 - val_dice_loss: 0.9974 - val_jaccard_loss: 0.9987 - val_focal_tversky_loss: 0.9986 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1280 - dice_score: 0.8720 - iou: 0.7776 - dice_loss: 0.1280 - jaccard_loss: 0.2184 - focal_tversky_loss: 0.2113\n",
      "Epoch 00008: val_loss did not improve from 0.44155\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 28s 309ms/step - loss: 0.1280 - dice_score: 0.8720 - iou: 0.7776 - dice_loss: 0.1280 - jaccard_loss: 0.2184 - focal_tversky_loss: 0.2113 - val_loss: 0.9396 - val_dice_score: 0.0604 - val_iou: 0.0318 - val_dice_loss: 0.9396 - val_jaccard_loss: 0.9682 - val_focal_tversky_loss: 0.9663 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1092 - dice_score: 0.8908 - iou: 0.8055 - dice_loss: 0.1092 - jaccard_loss: 0.1907 - focal_tversky_loss: 0.1921\n",
      "Epoch 00009: val_loss did not improve from 0.44155\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.1092 - dice_score: 0.8908 - iou: 0.8055 - dice_loss: 0.1092 - jaccard_loss: 0.1907 - focal_tversky_loss: 0.1921 - val_loss: 0.4921 - val_dice_score: 0.5079 - val_iou: 0.3538 - val_dice_loss: 0.4921 - val_jaccard_loss: 0.6443 - val_focal_tversky_loss: 0.6462 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1068 - dice_score: 0.8932 - iou: 0.8098 - dice_loss: 0.1068 - jaccard_loss: 0.1864 - focal_tversky_loss: 0.1903\n",
      "Epoch 00010: val_loss improved from 0.44155 to 0.26114, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 34s 374ms/step - loss: 0.1068 - dice_score: 0.8932 - iou: 0.8098 - dice_loss: 0.1068 - jaccard_loss: 0.1864 - focal_tversky_loss: 0.1903 - val_loss: 0.2611 - val_dice_score: 0.7389 - val_iou: 0.6014 - val_dice_loss: 0.2611 - val_jaccard_loss: 0.3955 - val_focal_tversky_loss: 0.4131 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0970 - dice_score: 0.9030 - iou: 0.8253 - dice_loss: 0.0970 - jaccard_loss: 0.1707 - focal_tversky_loss: 0.1746\n",
      "Epoch 00011: val_loss improved from 0.26114 to 0.13125, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 33s 373ms/step - loss: 0.0970 - dice_score: 0.9030 - iou: 0.8253 - dice_loss: 0.0970 - jaccard_loss: 0.1707 - focal_tversky_loss: 0.1746 - val_loss: 0.1313 - val_dice_score: 0.8687 - val_iou: 0.7719 - val_dice_loss: 0.1313 - val_jaccard_loss: 0.2244 - val_focal_tversky_loss: 0.2497 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0957 - dice_score: 0.9043 - iou: 0.8279 - dice_loss: 0.0957 - jaccard_loss: 0.1682 - focal_tversky_loss: 0.1726\n",
      "Epoch 00012: val_loss improved from 0.13125 to 0.09909, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 34s 376ms/step - loss: 0.0957 - dice_score: 0.9043 - iou: 0.8279 - dice_loss: 0.0957 - jaccard_loss: 0.1682 - focal_tversky_loss: 0.1726 - val_loss: 0.0991 - val_dice_score: 0.9009 - val_iou: 0.8213 - val_dice_loss: 0.0991 - val_jaccard_loss: 0.1746 - val_focal_tversky_loss: 0.1896 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0920 - dice_score: 0.9080 - iou: 0.8329 - dice_loss: 0.0920 - jaccard_loss: 0.1632 - focal_tversky_loss: 0.1713\n",
      "Epoch 00013: val_loss did not improve from 0.09909\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.0920 - dice_score: 0.9080 - iou: 0.8329 - dice_loss: 0.0920 - jaccard_loss: 0.1632 - focal_tversky_loss: 0.1713 - val_loss: 0.0996 - val_dice_score: 0.9004 - val_iou: 0.8205 - val_dice_loss: 0.0996 - val_jaccard_loss: 0.1753 - val_focal_tversky_loss: 0.1864 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0882 - dice_score: 0.9118 - iou: 0.8390 - dice_loss: 0.0882 - jaccard_loss: 0.1569 - focal_tversky_loss: 0.1626\n",
      "Epoch 00014: val_loss did not improve from 0.09909\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0882 - dice_score: 0.9118 - iou: 0.8390 - dice_loss: 0.0882 - jaccard_loss: 0.1569 - focal_tversky_loss: 0.1626 - val_loss: 0.1033 - val_dice_score: 0.8967 - val_iou: 0.8148 - val_dice_loss: 0.1033 - val_jaccard_loss: 0.1807 - val_focal_tversky_loss: 0.1735 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0887 - dice_score: 0.9113 - iou: 0.8384 - dice_loss: 0.0887 - jaccard_loss: 0.1575 - focal_tversky_loss: 0.1631\n",
      "Epoch 00015: val_loss did not improve from 0.09909\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.0887 - dice_score: 0.9113 - iou: 0.8384 - dice_loss: 0.0887 - jaccard_loss: 0.1575 - focal_tversky_loss: 0.1631 - val_loss: 0.1026 - val_dice_score: 0.8974 - val_iou: 0.8160 - val_dice_loss: 0.1026 - val_jaccard_loss: 0.1790 - val_focal_tversky_loss: 0.1700 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0822 - dice_score: 0.9178 - iou: 0.8491 - dice_loss: 0.0822 - jaccard_loss: 0.1468 - focal_tversky_loss: 0.1557\n",
      "Epoch 00016: val_loss did not improve from 0.09909\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0822 - dice_score: 0.9178 - iou: 0.8491 - dice_loss: 0.0822 - jaccard_loss: 0.1468 - focal_tversky_loss: 0.1557 - val_loss: 0.1247 - val_dice_score: 0.8753 - val_iou: 0.7831 - val_dice_loss: 0.1247 - val_jaccard_loss: 0.2128 - val_focal_tversky_loss: 0.1781 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0863 - dice_score: 0.9137 - iou: 0.8430 - dice_loss: 0.0863 - jaccard_loss: 0.1528 - focal_tversky_loss: 0.1591\n",
      "Epoch 00017: val_loss did not improve from 0.09909\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0863 - dice_score: 0.9137 - iou: 0.8430 - dice_loss: 0.0863 - jaccard_loss: 0.1528 - focal_tversky_loss: 0.1591 - val_loss: 0.1143 - val_dice_score: 0.8857 - val_iou: 0.7971 - val_dice_loss: 0.1143 - val_jaccard_loss: 0.1981 - val_focal_tversky_loss: 0.1667 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0845 - dice_score: 0.9155 - iou: 0.8453 - dice_loss: 0.0845 - jaccard_loss: 0.1504 - focal_tversky_loss: 0.1597\n",
      "Epoch 00018: val_loss did not improve from 0.09909\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0845 - dice_score: 0.9155 - iou: 0.8453 - dice_loss: 0.0845 - jaccard_loss: 0.1504 - focal_tversky_loss: 0.1597 - val_loss: 0.1058 - val_dice_score: 0.8942 - val_iou: 0.8150 - val_dice_loss: 0.1058 - val_jaccard_loss: 0.1793 - val_focal_tversky_loss: 0.1676 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0813 - dice_score: 0.9187 - iou: 0.8511 - dice_loss: 0.0813 - jaccard_loss: 0.1447 - focal_tversky_loss: 0.1528\n",
      "Epoch 00019: val_loss improved from 0.09909 to 0.09296, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 34s 376ms/step - loss: 0.0813 - dice_score: 0.9187 - iou: 0.8511 - dice_loss: 0.0813 - jaccard_loss: 0.1447 - focal_tversky_loss: 0.1528 - val_loss: 0.0930 - val_dice_score: 0.9070 - val_iou: 0.8311 - val_dice_loss: 0.0930 - val_jaccard_loss: 0.1633 - val_focal_tversky_loss: 0.1606 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0785 - dice_score: 0.9215 - iou: 0.8557 - dice_loss: 0.0785 - jaccard_loss: 0.1402 - focal_tversky_loss: 0.1498\n",
      "Epoch 00020: val_loss improved from 0.09296 to 0.09278, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 34s 382ms/step - loss: 0.0785 - dice_score: 0.9215 - iou: 0.8557 - dice_loss: 0.0785 - jaccard_loss: 0.1402 - focal_tversky_loss: 0.1498 - val_loss: 0.0928 - val_dice_score: 0.9072 - val_iou: 0.8314 - val_dice_loss: 0.0928 - val_jaccard_loss: 0.1632 - val_focal_tversky_loss: 0.1656 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0788 - dice_score: 0.9212 - iou: 0.8549 - dice_loss: 0.0788 - jaccard_loss: 0.1408 - focal_tversky_loss: 0.1490\n",
      "Epoch 00021: val_loss improved from 0.09278 to 0.09270, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 34s 376ms/step - loss: 0.0788 - dice_score: 0.9212 - iou: 0.8549 - dice_loss: 0.0788 - jaccard_loss: 0.1408 - focal_tversky_loss: 0.1490 - val_loss: 0.0927 - val_dice_score: 0.9073 - val_iou: 0.8328 - val_dice_loss: 0.0927 - val_jaccard_loss: 0.1619 - val_focal_tversky_loss: 0.1606 - lr: 1.0000e-05\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.0805 - dice_score: 0.9195 - iou: 0.8522 - dice_loss: 0.0805 - jaccard_loss: 0.1435 - focal_tversky_loss: 0.1529\n",
      "Epoch 00022: val_loss improved from 0.09270 to 0.08982, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 34s 375ms/step - loss: 0.0805 - dice_score: 0.9195 - iou: 0.8522 - dice_loss: 0.0805 - jaccard_loss: 0.1435 - focal_tversky_loss: 0.1529 - val_loss: 0.0898 - val_dice_score: 0.9102 - val_iou: 0.8364 - val_dice_loss: 0.0898 - val_jaccard_loss: 0.1587 - val_focal_tversky_loss: 0.1596 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0780 - dice_score: 0.9220 - iou: 0.8564 - dice_loss: 0.0780 - jaccard_loss: 0.1394 - focal_tversky_loss: 0.1473\n",
      "Epoch 00023: val_loss did not improve from 0.08982\n",
      "90/90 [==============================] - 28s 308ms/step - loss: 0.0780 - dice_score: 0.9220 - iou: 0.8564 - dice_loss: 0.0780 - jaccard_loss: 0.1394 - focal_tversky_loss: 0.1473 - val_loss: 0.0901 - val_dice_score: 0.9099 - val_iou: 0.8360 - val_dice_loss: 0.0901 - val_jaccard_loss: 0.1586 - val_focal_tversky_loss: 0.1590 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0798 - dice_score: 0.9202 - iou: 0.8534 - dice_loss: 0.0798 - jaccard_loss: 0.1423 - focal_tversky_loss: 0.1507\n",
      "Epoch 00024: val_loss improved from 0.08982 to 0.08609, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 34s 374ms/step - loss: 0.0798 - dice_score: 0.9202 - iou: 0.8534 - dice_loss: 0.0798 - jaccard_loss: 0.1423 - focal_tversky_loss: 0.1507 - val_loss: 0.0861 - val_dice_score: 0.9139 - val_iou: 0.8427 - val_dice_loss: 0.0861 - val_jaccard_loss: 0.1518 - val_focal_tversky_loss: 0.1533 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0778 - dice_score: 0.9222 - iou: 0.8565 - dice_loss: 0.0778 - jaccard_loss: 0.1394 - focal_tversky_loss: 0.1502\n",
      "Epoch 00025: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 28s 307ms/step - loss: 0.0778 - dice_score: 0.9222 - iou: 0.8565 - dice_loss: 0.0778 - jaccard_loss: 0.1394 - focal_tversky_loss: 0.1502 - val_loss: 0.0882 - val_dice_score: 0.9118 - val_iou: 0.8393 - val_dice_loss: 0.0882 - val_jaccard_loss: 0.1559 - val_focal_tversky_loss: 0.1599 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0756 - dice_score: 0.9244 - iou: 0.8603 - dice_loss: 0.0756 - jaccard_loss: 0.1356 - focal_tversky_loss: 0.1439\n",
      "Epoch 00026: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0756 - dice_score: 0.9244 - iou: 0.8603 - dice_loss: 0.0756 - jaccard_loss: 0.1356 - focal_tversky_loss: 0.1439 - val_loss: 0.0876 - val_dice_score: 0.9124 - val_iou: 0.8402 - val_dice_loss: 0.0876 - val_jaccard_loss: 0.1552 - val_focal_tversky_loss: 0.1587 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0758 - dice_score: 0.9242 - iou: 0.8600 - dice_loss: 0.0758 - jaccard_loss: 0.1359 - focal_tversky_loss: 0.1441\n",
      "Epoch 00027: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.0758 - dice_score: 0.9242 - iou: 0.8600 - dice_loss: 0.0758 - jaccard_loss: 0.1359 - focal_tversky_loss: 0.1441 - val_loss: 0.0910 - val_dice_score: 0.9090 - val_iou: 0.8349 - val_dice_loss: 0.0910 - val_jaccard_loss: 0.1600 - val_focal_tversky_loss: 0.1590 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0784 - dice_score: 0.9216 - iou: 0.8557 - dice_loss: 0.0784 - jaccard_loss: 0.1400 - focal_tversky_loss: 0.1490\n",
      "Epoch 00028: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0784 - dice_score: 0.9216 - iou: 0.8557 - dice_loss: 0.0784 - jaccard_loss: 0.1400 - focal_tversky_loss: 0.1490 - val_loss: 0.0898 - val_dice_score: 0.9102 - val_iou: 0.8367 - val_dice_loss: 0.0898 - val_jaccard_loss: 0.1589 - val_focal_tversky_loss: 0.1616 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0809 - dice_score: 0.9191 - iou: 0.8520 - dice_loss: 0.0809 - jaccard_loss: 0.1437 - focal_tversky_loss: 0.1518\n",
      "Epoch 00029: val_loss did not improve from 0.08609\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0809 - dice_score: 0.9191 - iou: 0.8520 - dice_loss: 0.0809 - jaccard_loss: 0.1437 - focal_tversky_loss: 0.1518 - val_loss: 0.0880 - val_dice_score: 0.9120 - val_iou: 0.8391 - val_dice_loss: 0.0880 - val_jaccard_loss: 0.1566 - val_focal_tversky_loss: 0.1574 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0766 - dice_score: 0.9234 - iou: 0.8589 - dice_loss: 0.0766 - jaccard_loss: 0.1369 - focal_tversky_loss: 0.1454\n",
      "Epoch 00030: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0766 - dice_score: 0.9234 - iou: 0.8589 - dice_loss: 0.0766 - jaccard_loss: 0.1369 - focal_tversky_loss: 0.1454 - val_loss: 0.0870 - val_dice_score: 0.9130 - val_iou: 0.8406 - val_dice_loss: 0.0870 - val_jaccard_loss: 0.1551 - val_focal_tversky_loss: 0.1575 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0761 - dice_score: 0.9239 - iou: 0.8596 - dice_loss: 0.0761 - jaccard_loss: 0.1362 - focal_tversky_loss: 0.1447\n",
      "Epoch 00031: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0761 - dice_score: 0.9239 - iou: 0.8596 - dice_loss: 0.0761 - jaccard_loss: 0.1362 - focal_tversky_loss: 0.1447 - val_loss: 0.0870 - val_dice_score: 0.9130 - val_iou: 0.8411 - val_dice_loss: 0.0870 - val_jaccard_loss: 0.1547 - val_focal_tversky_loss: 0.1603 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0778 - dice_score: 0.9222 - iou: 0.8568 - dice_loss: 0.0778 - jaccard_loss: 0.1389 - focal_tversky_loss: 0.1478\n",
      "Epoch 00032: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.0778 - dice_score: 0.9222 - iou: 0.8568 - dice_loss: 0.0778 - jaccard_loss: 0.1389 - focal_tversky_loss: 0.1478 - val_loss: 0.0908 - val_dice_score: 0.9092 - val_iou: 0.8347 - val_dice_loss: 0.0908 - val_jaccard_loss: 0.1611 - val_focal_tversky_loss: 0.1605 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0751 - dice_score: 0.9249 - iou: 0.8612 - dice_loss: 0.0751 - jaccard_loss: 0.1344 - focal_tversky_loss: 0.1450\n",
      "Epoch 00033: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 299ms/step - loss: 0.0751 - dice_score: 0.9249 - iou: 0.8612 - dice_loss: 0.0751 - jaccard_loss: 0.1344 - focal_tversky_loss: 0.1450 - val_loss: 0.0879 - val_dice_score: 0.9121 - val_iou: 0.8392 - val_dice_loss: 0.0879 - val_jaccard_loss: 0.1565 - val_focal_tversky_loss: 0.1587 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0785 - dice_score: 0.9215 - iou: 0.8557 - dice_loss: 0.0785 - jaccard_loss: 0.1400 - focal_tversky_loss: 0.1505\n",
      "Epoch 00034: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0785 - dice_score: 0.9215 - iou: 0.8557 - dice_loss: 0.0785 - jaccard_loss: 0.1400 - focal_tversky_loss: 0.1505 - val_loss: 0.0865 - val_dice_score: 0.9135 - val_iou: 0.8420 - val_dice_loss: 0.0865 - val_jaccard_loss: 0.1538 - val_focal_tversky_loss: 0.1564 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0773 - dice_score: 0.9227 - iou: 0.8575 - dice_loss: 0.0773 - jaccard_loss: 0.1383 - focal_tversky_loss: 0.1455\n",
      "Epoch 00035: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.0773 - dice_score: 0.9227 - iou: 0.8575 - dice_loss: 0.0773 - jaccard_loss: 0.1383 - focal_tversky_loss: 0.1455 - val_loss: 0.0915 - val_dice_score: 0.9085 - val_iou: 0.8340 - val_dice_loss: 0.0915 - val_jaccard_loss: 0.1618 - val_focal_tversky_loss: 0.1597 - lr: 1.0000e-05\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.0770 - dice_score: 0.9230 - iou: 0.8582 - dice_loss: 0.0770 - jaccard_loss: 0.1376 - focal_tversky_loss: 0.1470\n",
      "Epoch 00036: val_loss did not improve from 0.08609\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0770 - dice_score: 0.9230 - iou: 0.8582 - dice_loss: 0.0770 - jaccard_loss: 0.1376 - focal_tversky_loss: 0.1470 - val_loss: 0.0885 - val_dice_score: 0.9115 - val_iou: 0.8388 - val_dice_loss: 0.0885 - val_jaccard_loss: 0.1567 - val_focal_tversky_loss: 0.1585 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0799 - dice_score: 0.9201 - iou: 0.8534 - dice_loss: 0.0799 - jaccard_loss: 0.1419 - focal_tversky_loss: 0.1492\n",
      "Epoch 00037: val_loss improved from 0.08609 to 0.08299, saving model to saved_models/ISIC_2016/unet_res50/usual_1/exp_2/model\n",
      "90/90 [==============================] - 34s 375ms/step - loss: 0.0799 - dice_score: 0.9201 - iou: 0.8534 - dice_loss: 0.0799 - jaccard_loss: 0.1419 - focal_tversky_loss: 0.1492 - val_loss: 0.0830 - val_dice_score: 0.9170 - val_iou: 0.8475 - val_dice_loss: 0.0830 - val_jaccard_loss: 0.1479 - val_focal_tversky_loss: 0.1523 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0764 - dice_score: 0.9236 - iou: 0.8589 - dice_loss: 0.0764 - jaccard_loss: 0.1367 - focal_tversky_loss: 0.1466\n",
      "Epoch 00038: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0764 - dice_score: 0.9236 - iou: 0.8589 - dice_loss: 0.0764 - jaccard_loss: 0.1367 - focal_tversky_loss: 0.1466 - val_loss: 0.0889 - val_dice_score: 0.9111 - val_iou: 0.8385 - val_dice_loss: 0.0889 - val_jaccard_loss: 0.1573 - val_focal_tversky_loss: 0.1563 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0745 - dice_score: 0.9255 - iou: 0.8623 - dice_loss: 0.0745 - jaccard_loss: 0.1334 - focal_tversky_loss: 0.1418\n",
      "Epoch 00039: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 28s 309ms/step - loss: 0.0745 - dice_score: 0.9255 - iou: 0.8623 - dice_loss: 0.0745 - jaccard_loss: 0.1334 - focal_tversky_loss: 0.1418 - val_loss: 0.0892 - val_dice_score: 0.9108 - val_iou: 0.8381 - val_dice_loss: 0.0892 - val_jaccard_loss: 0.1576 - val_focal_tversky_loss: 0.1544 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0756 - dice_score: 0.9244 - iou: 0.8602 - dice_loss: 0.0756 - jaccard_loss: 0.1353 - focal_tversky_loss: 0.1457\n",
      "Epoch 00040: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0756 - dice_score: 0.9244 - iou: 0.8602 - dice_loss: 0.0756 - jaccard_loss: 0.1353 - focal_tversky_loss: 0.1457 - val_loss: 0.0857 - val_dice_score: 0.9143 - val_iou: 0.8433 - val_dice_loss: 0.0857 - val_jaccard_loss: 0.1524 - val_focal_tversky_loss: 0.1529 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0751 - dice_score: 0.9249 - iou: 0.8613 - dice_loss: 0.0751 - jaccard_loss: 0.1343 - focal_tversky_loss: 0.1431\n",
      "Epoch 00041: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0751 - dice_score: 0.9249 - iou: 0.8613 - dice_loss: 0.0751 - jaccard_loss: 0.1343 - focal_tversky_loss: 0.1431 - val_loss: 0.0874 - val_dice_score: 0.9126 - val_iou: 0.8399 - val_dice_loss: 0.0874 - val_jaccard_loss: 0.1557 - val_focal_tversky_loss: 0.1551 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0767 - dice_score: 0.9233 - iou: 0.8586 - dice_loss: 0.0767 - jaccard_loss: 0.1369 - focal_tversky_loss: 0.1472\n",
      "Epoch 00042: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0767 - dice_score: 0.9233 - iou: 0.8586 - dice_loss: 0.0767 - jaccard_loss: 0.1369 - focal_tversky_loss: 0.1472 - val_loss: 0.0895 - val_dice_score: 0.9105 - val_iou: 0.8371 - val_dice_loss: 0.0895 - val_jaccard_loss: 0.1585 - val_focal_tversky_loss: 0.1582 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0758 - dice_score: 0.9242 - iou: 0.8598 - dice_loss: 0.0758 - jaccard_loss: 0.1355 - focal_tversky_loss: 0.1464\n",
      "Epoch 00043: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0758 - dice_score: 0.9242 - iou: 0.8598 - dice_loss: 0.0758 - jaccard_loss: 0.1355 - focal_tversky_loss: 0.1464 - val_loss: 0.0880 - val_dice_score: 0.9120 - val_iou: 0.8392 - val_dice_loss: 0.0880 - val_jaccard_loss: 0.1566 - val_focal_tversky_loss: 0.1561 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0742 - dice_score: 0.9258 - iou: 0.8626 - dice_loss: 0.0742 - jaccard_loss: 0.1329 - focal_tversky_loss: 0.1422\n",
      "Epoch 00044: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.0742 - dice_score: 0.9258 - iou: 0.8626 - dice_loss: 0.0742 - jaccard_loss: 0.1329 - focal_tversky_loss: 0.1422 - val_loss: 0.0896 - val_dice_score: 0.9104 - val_iou: 0.8366 - val_dice_loss: 0.0896 - val_jaccard_loss: 0.1590 - val_focal_tversky_loss: 0.1563 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0761 - dice_score: 0.9239 - iou: 0.8598 - dice_loss: 0.0761 - jaccard_loss: 0.1357 - focal_tversky_loss: 0.1466\n",
      "Epoch 00045: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 300ms/step - loss: 0.0761 - dice_score: 0.9239 - iou: 0.8598 - dice_loss: 0.0761 - jaccard_loss: 0.1357 - focal_tversky_loss: 0.1466 - val_loss: 0.0875 - val_dice_score: 0.9125 - val_iou: 0.8405 - val_dice_loss: 0.0875 - val_jaccard_loss: 0.1552 - val_focal_tversky_loss: 0.1563 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0740 - dice_score: 0.9260 - iou: 0.8632 - dice_loss: 0.0740 - jaccard_loss: 0.1325 - focal_tversky_loss: 0.1425\n",
      "Epoch 00046: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0740 - dice_score: 0.9260 - iou: 0.8632 - dice_loss: 0.0740 - jaccard_loss: 0.1325 - focal_tversky_loss: 0.1425 - val_loss: 0.0857 - val_dice_score: 0.9143 - val_iou: 0.8438 - val_dice_loss: 0.0857 - val_jaccard_loss: 0.1521 - val_focal_tversky_loss: 0.1525 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0767 - dice_score: 0.9233 - iou: 0.8587 - dice_loss: 0.0767 - jaccard_loss: 0.1368 - focal_tversky_loss: 0.1464\n",
      "Epoch 00047: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.0767 - dice_score: 0.9233 - iou: 0.8587 - dice_loss: 0.0767 - jaccard_loss: 0.1368 - focal_tversky_loss: 0.1464 - val_loss: 0.0897 - val_dice_score: 0.9103 - val_iou: 0.8369 - val_dice_loss: 0.0897 - val_jaccard_loss: 0.1590 - val_focal_tversky_loss: 0.1598 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0740 - dice_score: 0.9260 - iou: 0.8631 - dice_loss: 0.0740 - jaccard_loss: 0.1326 - focal_tversky_loss: 0.1412\n",
      "Epoch 00048: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0740 - dice_score: 0.9260 - iou: 0.8631 - dice_loss: 0.0740 - jaccard_loss: 0.1326 - focal_tversky_loss: 0.1412 - val_loss: 0.0928 - val_dice_score: 0.9072 - val_iou: 0.8320 - val_dice_loss: 0.0928 - val_jaccard_loss: 0.1637 - val_focal_tversky_loss: 0.1595 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0735 - dice_score: 0.9265 - iou: 0.8640 - dice_loss: 0.0735 - jaccard_loss: 0.1315 - focal_tversky_loss: 0.1423\n",
      "Epoch 00049: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0735 - dice_score: 0.9265 - iou: 0.8640 - dice_loss: 0.0735 - jaccard_loss: 0.1315 - focal_tversky_loss: 0.1423 - val_loss: 0.0860 - val_dice_score: 0.9140 - val_iou: 0.8426 - val_dice_loss: 0.0860 - val_jaccard_loss: 0.1533 - val_focal_tversky_loss: 0.1572 - lr: 1.0000e-05\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.0748 - dice_score: 0.9252 - iou: 0.8621 - dice_loss: 0.0748 - jaccard_loss: 0.1338 - focal_tversky_loss: 0.1435\n",
      "Epoch 00050: val_loss did not improve from 0.08299\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0748 - dice_score: 0.9252 - iou: 0.8621 - dice_loss: 0.0748 - jaccard_loss: 0.1338 - focal_tversky_loss: 0.1435 - val_loss: 0.0883 - val_dice_score: 0.9117 - val_iou: 0.8389 - val_dice_loss: 0.0883 - val_jaccard_loss: 0.1570 - val_focal_tversky_loss: 0.1572 - lr: 1.0000e-05\n",
      "2022-02-26 22:45:34,957 - SKIN-CANCER - INFO - Training is done model is saved in saved_models/ISIC_2016/unet_res50/usual_1/exp_2\n",
      "Evaluation\n",
      "48/48 [==============================] - 8s 175ms/step - loss: 0.0906 - dice_score: 0.9094 - iou: 0.8387 - dice_loss: 0.0906 - jaccard_loss: 0.1573 - focal_tversky_loss: 0.1553\n",
      "2022-02-26 22:45:43,370 - SKIN-CANCER - INFO - Test: Loss= 0.09060574322938919, Dice-Score: 0.9093945026397705, IoU: 0.8386779427528381, Dice_loss: 0.09060574322938919, jaccard_loss: 0.15732716023921967, focal_tversky_loss: 0.15530382096767426\n",
      "1/1 [==============================] - 1s 737ms/step - loss: 0.1744 - dice_score: 0.8256 - iou: 0.7030 - dice_loss: 0.1744 - jaccard_loss: 0.2900 - focal_tversky_loss: 0.2292\n",
      "2022-02-26 22:45:51,706 - SKIN-CANCER - INFO - Successfully Saved figures!\n",
      "\n",
      "-----------------------------train 1 is done! ----------------------------------\n",
      "\n",
      "/home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/deep_utils/vision/face_detection/ultralight/__init__.py:3: UserWarning: \n",
      "No module named 'deep_utils.vision.face_detection.ultralight.torch'. If you don't use UltralightTorchFaceDetector ignore this message.\n",
      "  UltralightTorchFaceDetector = import_module(\n",
      "/home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/deep_utils/vision/face_detection/ultralight/__init__.py:6: UserWarning: \n",
      "No module named 'deep_utils.vision.face_detection.ultralight.tf'. If you don't use UltralightTFFaceDetector ignore this message.\n",
      "  UltralightTFFaceDetector = import_module(\n",
      "2022-02-26 22:45:55.900856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:45:55.937784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:45:55.937902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "[INFO] Successfully created logger for SKIN-CANCER\n",
      "[INFO] Saving params!\n",
      "2022-02-26 22:45:55,939 - SKIN-CANCER - INFO - [INFO] Params are successfully saved in saved_models/ISIC_2016/unet_res50/usual_2/exp_2/params.txt!\n",
      "2022-02-26 22:45:55,939 - SKIN-CANCER - INFO - Chosen Model: unet_res50\n",
      "2022-02-26 22:45:55,939 - SKIN-CANCER - INFO -  Arguments: Namespace(optimizer='adam', batch_size=8, img_channel=3, transfer_learning_epochs=5, finetuning_epochs=10, epochs=50, lr=0.001, min_lr=1e-05, reduce_lr_patience=5, reduce_lr_factor=0.1, early_stopping_p=20, mlflow_source='./mlruns', cutmix_p=0.0, cutmix_beta=0.0, usual_aug_with_cutmix=False, hair_aug_p=0.0, hair_rmv_p=0.0, random_rotate_p=0.5, p_horizontal_flip=0.5, p_vertical_flip=0.5, p_center_crop=0.5, mosaic_p=0.0, loss='dice_loss', label_smoothing=0, focal_loss_gamma=2, pos_weight=1, neg_weight=1, train_path='data/train', test_path='data/test', mask_train_path='data/mask_train', mask_test_path='data/mask_test', val_path='data/val', mask_val_path='data/mask_val', verbose=1, save_path='saved_models/', dataset_name='ISIC_2016', seed=1236, img_size=(256, 256), save_path_name='usual_2', model='unet_res50')\n",
      "[INFO] Successfully created MLFLOW-Handler\n",
      "2022-02-26 22:45:56,009 - SKIN-CANCER - INFO - Data Loader is successfully loaded!\n",
      "2022-02-26 22:45:56.017762: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-26 22:45:56.017931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:45:56.018034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:45:56.018083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:45:56.565809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:45:56.565927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:45:56.565984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 22:45:56.566577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21752 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Model: \"ResNet50_U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 262, 262, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 128, 128, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 128, 128, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 128, 128, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 130, 130, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 64, 64, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 64, 64, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 64, 64, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 64, 64, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 32, 32, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 32, 32, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 32, 32, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 16, 16, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 16, 16, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 16, 16, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 16, 16, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 16, 16, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 32, 32, 512)  2097664    ['conv4_block6_out[0][0]']       \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 1024  0           ['conv2d_transpose[0][0]',       \n",
      "                                )                                 'conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 512)  4719104     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 512)  2048       ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 512)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 512)  2359808     ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 256)  524544     ['activation_1[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 512)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 256)  1179904     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 64, 64, 256)  590080      ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 12  131200     ['activation_3[0][0]']           \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                2)                                'conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 128, 128, 12  221312      ['concatenate_2[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 128, 128, 12  512        ['conv2d_4[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_4[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 128, 128, 12  147584      ['activation_4[0][0]']           \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128, 128, 12  512        ['conv2d_5[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_5[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 64  32832      ['activation_5[0][0]']           \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256, 256, 67  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                )                                 'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 256, 256, 64  38656       ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 256, 256, 64  256        ['conv2d_6[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 256, 256, 64  0           ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 256, 256, 64  36928       ['activation_6[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 256, 256, 64  256        ['conv2d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 256, 256, 64  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 256, 256, 1)  65          ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,676,545\n",
      "Trainable params: 20,642,113\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "2022-02-26 22:45:57,219 - SKIN-CANCER - INFO - model: unet_res50 is successfully loaded!\n",
      "2022-02-26 22:45:57.221068: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-02-26 22:45:57.221080: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-02-26 22:45:57.221336: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1630] Profiler found 1 GPUs\n",
      "2022-02-26 22:45:57.221748: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ai/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.6/lib64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-26 22:45:57.286740: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-02-26 22:45:57.287093: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "[INFO] Successfully created following callbacks [model_checkpoint, reduce_lr, early_stop, csv_logger, tensorboard]\n",
      "2022-02-26 22:45:57,289 - SKIN-CANCER - INFO - loss: dice_loss is successfully created!\n",
      "2022-02-26 22:45:57,293 - SKIN-CANCER - INFO - Start train for saved_models/ISIC_2016/unet_res50/usual_2/exp_2\n",
      "Epoch 1/50\n",
      "2022-02-26 22:46:00.522873: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8301\n",
      " 1/90 [..............................] - ETA: 7:39 - loss: 0.6676 - dice_score: 0.3324 - iou: 0.1993 - dice_loss: 0.6676 - jaccard_loss: 0.6954 - focal_tversky_loss: 0.68872022-02-26 22:46:02.782549: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-02-26 22:46:02.782571: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-02-26 22:46:03.358539: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-02-26 22:46:03.358874: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-02-26 22:46:03.374721: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 1686 callback api events and 1694 activity events. \n",
      "2022-02-26 22:46:03.387293: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-02-26 22:46:03.406912: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03\n",
      "\n",
      "2022-02-26 22:46:03.420571: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03/ai.trace.json.gz\n",
      "2022-02-26 22:46:03.447416: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03\n",
      "\n",
      "2022-02-26 22:46:03.450909: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03/ai.memory_profile.json.gz\n",
      "2022-02-26 22:46:03.452009: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03\n",
      "Dumped tool data for xplane.pb to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03/ai.xplane.pb\n",
      "Dumped tool data for overview_page.pb to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03/ai.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03/ai.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03/ai.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/tensorboard_train/train/plugins/profile/2022_02_26_22_46_03/ai.kernel_stats.pb\n",
      "\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2040 - dice_score: 0.7960 - iou: 0.6723 - dice_loss: 0.2040 - jaccard_loss: 0.2766 - focal_tversky_loss: 0.2821\n",
      "Epoch 00001: val_loss improved from inf to 0.99620, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "2022-02-26 22:46:32.701129: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "90/90 [==============================] - 39s 378ms/step - loss: 0.2040 - dice_score: 0.7960 - iou: 0.6723 - dice_loss: 0.2040 - jaccard_loss: 0.2766 - focal_tversky_loss: 0.2821 - val_loss: 0.9962 - val_dice_score: 0.0038 - val_iou: 0.0019 - val_dice_loss: 0.9962 - val_jaccard_loss: 0.9981 - val_focal_tversky_loss: 0.9980 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1630 - dice_score: 0.8370 - iou: 0.7242 - dice_loss: 0.1630 - jaccard_loss: 0.2579 - focal_tversky_loss: 0.2514\n",
      "Epoch 00002: val_loss improved from 0.99620 to 0.93821, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 370ms/step - loss: 0.1630 - dice_score: 0.8370 - iou: 0.7242 - dice_loss: 0.1630 - jaccard_loss: 0.2579 - focal_tversky_loss: 0.2514 - val_loss: 0.9382 - val_dice_score: 0.0618 - val_iou: 0.0322 - val_dice_loss: 0.9382 - val_jaccard_loss: 0.9670 - val_focal_tversky_loss: 0.9657 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1469 - dice_score: 0.8531 - iou: 0.7482 - dice_loss: 0.1469 - jaccard_loss: 0.2425 - focal_tversky_loss: 0.2359\n",
      "Epoch 00003: val_loss did not improve from 0.93821\n",
      "90/90 [==============================] - 27s 300ms/step - loss: 0.1469 - dice_score: 0.8531 - iou: 0.7482 - dice_loss: 0.1469 - jaccard_loss: 0.2425 - focal_tversky_loss: 0.2359 - val_loss: 0.9982 - val_dice_score: 0.0018 - val_iou: 9.1340e-04 - val_dice_loss: 0.9982 - val_jaccard_loss: 0.9991 - val_focal_tversky_loss: 0.9990 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1335 - dice_score: 0.8665 - iou: 0.7683 - dice_loss: 0.1335 - jaccard_loss: 0.2244 - focal_tversky_loss: 0.2202\n",
      "Epoch 00004: val_loss did not improve from 0.93821\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.1335 - dice_score: 0.8665 - iou: 0.7683 - dice_loss: 0.1335 - jaccard_loss: 0.2244 - focal_tversky_loss: 0.2202 - val_loss: 0.9989 - val_dice_score: 0.0011 - val_iou: 5.2629e-04 - val_dice_loss: 0.9989 - val_jaccard_loss: 0.9995 - val_focal_tversky_loss: 0.9994 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1267 - dice_score: 0.8733 - iou: 0.7788 - dice_loss: 0.1267 - jaccard_loss: 0.2149 - focal_tversky_loss: 0.2121\n",
      "Epoch 00005: val_loss did not improve from 0.93821\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.1267 - dice_score: 0.8733 - iou: 0.7788 - dice_loss: 0.1267 - jaccard_loss: 0.2149 - focal_tversky_loss: 0.2121 - val_loss: 0.9760 - val_dice_score: 0.0240 - val_iou: 0.0122 - val_dice_loss: 0.9760 - val_jaccard_loss: 0.9877 - val_focal_tversky_loss: 0.9869 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1214 - dice_score: 0.8786 - iou: 0.7864 - dice_loss: 0.1214 - jaccard_loss: 0.2082 - focal_tversky_loss: 0.2063\n",
      "Epoch 00006: val_loss improved from 0.93821 to 0.91976, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 371ms/step - loss: 0.1214 - dice_score: 0.8786 - iou: 0.7864 - dice_loss: 0.1214 - jaccard_loss: 0.2082 - focal_tversky_loss: 0.2063 - val_loss: 0.9198 - val_dice_score: 0.0802 - val_iou: 0.0439 - val_dice_loss: 0.9198 - val_jaccard_loss: 0.9559 - val_focal_tversky_loss: 0.9535 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1149 - dice_score: 0.8851 - iou: 0.7962 - dice_loss: 0.1149 - jaccard_loss: 0.1991 - focal_tversky_loss: 0.1996\n",
      "Epoch 00007: val_loss did not improve from 0.91976\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.1149 - dice_score: 0.8851 - iou: 0.7962 - dice_loss: 0.1149 - jaccard_loss: 0.1991 - focal_tversky_loss: 0.1996 - val_loss: 0.9872 - val_dice_score: 0.0128 - val_iou: 0.0066 - val_dice_loss: 0.9872 - val_jaccard_loss: 0.9934 - val_focal_tversky_loss: 0.9930 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1047 - dice_score: 0.8953 - iou: 0.8131 - dice_loss: 0.1047 - jaccard_loss: 0.1826 - focal_tversky_loss: 0.1859\n",
      "Epoch 00008: val_loss improved from 0.91976 to 0.91754, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 33s 370ms/step - loss: 0.1047 - dice_score: 0.8953 - iou: 0.8131 - dice_loss: 0.1047 - jaccard_loss: 0.1826 - focal_tversky_loss: 0.1859 - val_loss: 0.9175 - val_dice_score: 0.0825 - val_iou: 0.0438 - val_dice_loss: 0.9175 - val_jaccard_loss: 0.9561 - val_focal_tversky_loss: 0.9535 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1210 - dice_score: 0.8790 - iou: 0.7868 - dice_loss: 0.1210 - jaccard_loss: 0.2092 - focal_tversky_loss: 0.2032\n",
      "Epoch 00009: val_loss improved from 0.91754 to 0.61587, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 369ms/step - loss: 0.1210 - dice_score: 0.8790 - iou: 0.7868 - dice_loss: 0.1210 - jaccard_loss: 0.2092 - focal_tversky_loss: 0.2032 - val_loss: 0.6159 - val_dice_score: 0.3841 - val_iou: 0.2556 - val_dice_loss: 0.6159 - val_jaccard_loss: 0.7432 - val_focal_tversky_loss: 0.7401 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1094 - dice_score: 0.8906 - iou: 0.8050 - dice_loss: 0.1094 - jaccard_loss: 0.1911 - focal_tversky_loss: 0.1920\n",
      "Epoch 00010: val_loss improved from 0.61587 to 0.17188, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 371ms/step - loss: 0.1094 - dice_score: 0.8906 - iou: 0.8050 - dice_loss: 0.1094 - jaccard_loss: 0.1911 - focal_tversky_loss: 0.1920 - val_loss: 0.1719 - val_dice_score: 0.8281 - val_iou: 0.7106 - val_dice_loss: 0.1719 - val_jaccard_loss: 0.2864 - val_focal_tversky_loss: 0.2386 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1008 - dice_score: 0.8992 - iou: 0.8188 - dice_loss: 0.1008 - jaccard_loss: 0.1773 - focal_tversky_loss: 0.1822\n",
      "Epoch 00011: val_loss did not improve from 0.17188\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.1008 - dice_score: 0.8992 - iou: 0.8188 - dice_loss: 0.1008 - jaccard_loss: 0.1773 - focal_tversky_loss: 0.1822 - val_loss: 0.2281 - val_dice_score: 0.7719 - val_iou: 0.6449 - val_dice_loss: 0.2281 - val_jaccard_loss: 0.3523 - val_focal_tversky_loss: 0.3598 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1073 - dice_score: 0.8927 - iou: 0.8087 - dice_loss: 0.1073 - jaccard_loss: 0.1879 - focal_tversky_loss: 0.1866\n",
      "Epoch 00012: val_loss did not improve from 0.17188\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.1073 - dice_score: 0.8927 - iou: 0.8087 - dice_loss: 0.1073 - jaccard_loss: 0.1879 - focal_tversky_loss: 0.1866 - val_loss: 0.2789 - val_dice_score: 0.7211 - val_iou: 0.5687 - val_dice_loss: 0.2789 - val_jaccard_loss: 0.4285 - val_focal_tversky_loss: 0.4259 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1172 - dice_score: 0.8828 - iou: 0.7933 - dice_loss: 0.1172 - jaccard_loss: 0.2034 - focal_tversky_loss: 0.2025\n",
      "Epoch 00013: val_loss improved from 0.17188 to 0.13364, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 372ms/step - loss: 0.1172 - dice_score: 0.8828 - iou: 0.7933 - dice_loss: 0.1172 - jaccard_loss: 0.2034 - focal_tversky_loss: 0.2025 - val_loss: 0.1336 - val_dice_score: 0.8664 - val_iou: 0.7656 - val_dice_loss: 0.1336 - val_jaccard_loss: 0.2319 - val_focal_tversky_loss: 0.2081 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1067 - dice_score: 0.8933 - iou: 0.8095 - dice_loss: 0.1067 - jaccard_loss: 0.1876 - focal_tversky_loss: 0.1866\n",
      "Epoch 00014: val_loss improved from 0.13364 to 0.12438, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 369ms/step - loss: 0.1067 - dice_score: 0.8933 - iou: 0.8095 - dice_loss: 0.1067 - jaccard_loss: 0.1876 - focal_tversky_loss: 0.1866 - val_loss: 0.1244 - val_dice_score: 0.8756 - val_iou: 0.7867 - val_dice_loss: 0.1244 - val_jaccard_loss: 0.2110 - val_focal_tversky_loss: 0.2109 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0951 - dice_score: 0.9049 - iou: 0.8279 - dice_loss: 0.0951 - jaccard_loss: 0.1691 - focal_tversky_loss: 0.1732\n",
      "Epoch 00015: val_loss did not improve from 0.12438\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0951 - dice_score: 0.9049 - iou: 0.8279 - dice_loss: 0.0951 - jaccard_loss: 0.1691 - focal_tversky_loss: 0.1732 - val_loss: 0.2177 - val_dice_score: 0.7823 - val_iou: 0.6511 - val_dice_loss: 0.2177 - val_jaccard_loss: 0.3467 - val_focal_tversky_loss: 0.3571 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0975 - dice_score: 0.9025 - iou: 0.8241 - dice_loss: 0.0975 - jaccard_loss: 0.1731 - focal_tversky_loss: 0.1757\n",
      "Epoch 00016: val_loss did not improve from 0.12438\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.0975 - dice_score: 0.9025 - iou: 0.8241 - dice_loss: 0.0975 - jaccard_loss: 0.1731 - focal_tversky_loss: 0.1757 - val_loss: 0.1707 - val_dice_score: 0.8293 - val_iou: 0.7119 - val_dice_loss: 0.1707 - val_jaccard_loss: 0.2858 - val_focal_tversky_loss: 0.3124 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0927 - dice_score: 0.9073 - iou: 0.8319 - dice_loss: 0.0927 - jaccard_loss: 0.1653 - focal_tversky_loss: 0.1712\n",
      "Epoch 00017: val_loss improved from 0.12438 to 0.11443, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 34s 375ms/step - loss: 0.0927 - dice_score: 0.9073 - iou: 0.8319 - dice_loss: 0.0927 - jaccard_loss: 0.1653 - focal_tversky_loss: 0.1712 - val_loss: 0.1144 - val_dice_score: 0.8856 - val_iou: 0.7960 - val_dice_loss: 0.1144 - val_jaccard_loss: 0.2019 - val_focal_tversky_loss: 0.1942 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0996 - dice_score: 0.9004 - iou: 0.8215 - dice_loss: 0.0996 - jaccard_loss: 0.1759 - focal_tversky_loss: 0.1778\n",
      "Epoch 00018: val_loss did not improve from 0.11443\n",
      "90/90 [==============================] - 28s 309ms/step - loss: 0.0996 - dice_score: 0.9004 - iou: 0.8215 - dice_loss: 0.0996 - jaccard_loss: 0.1759 - focal_tversky_loss: 0.1778 - val_loss: 0.1491 - val_dice_score: 0.8509 - val_iou: 0.7483 - val_dice_loss: 0.1491 - val_jaccard_loss: 0.2496 - val_focal_tversky_loss: 0.2691 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0929 - dice_score: 0.9071 - iou: 0.8316 - dice_loss: 0.0929 - jaccard_loss: 0.1658 - focal_tversky_loss: 0.1679\n",
      "Epoch 00019: val_loss did not improve from 0.11443\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.0929 - dice_score: 0.9071 - iou: 0.8316 - dice_loss: 0.0929 - jaccard_loss: 0.1658 - focal_tversky_loss: 0.1679 - val_loss: 0.1910 - val_dice_score: 0.8090 - val_iou: 0.6909 - val_dice_loss: 0.1910 - val_jaccard_loss: 0.3072 - val_focal_tversky_loss: 0.3099 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1010 - dice_score: 0.8990 - iou: 0.8189 - dice_loss: 0.1010 - jaccard_loss: 0.1785 - focal_tversky_loss: 0.1808\n",
      "Epoch 00020: val_loss did not improve from 0.11443\n",
      "90/90 [==============================] - 28s 312ms/step - loss: 0.1010 - dice_score: 0.8990 - iou: 0.8189 - dice_loss: 0.1010 - jaccard_loss: 0.1785 - focal_tversky_loss: 0.1808 - val_loss: 0.1454 - val_dice_score: 0.8546 - val_iou: 0.7523 - val_dice_loss: 0.1454 - val_jaccard_loss: 0.2455 - val_focal_tversky_loss: 0.2396 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0987 - dice_score: 0.9013 - iou: 0.8227 - dice_loss: 0.0987 - jaccard_loss: 0.1748 - focal_tversky_loss: 0.1773\n",
      "Epoch 00021: val_loss did not improve from 0.11443\n",
      "90/90 [==============================] - 28s 315ms/step - loss: 0.0987 - dice_score: 0.9013 - iou: 0.8227 - dice_loss: 0.0987 - jaccard_loss: 0.1748 - focal_tversky_loss: 0.1773 - val_loss: 0.2607 - val_dice_score: 0.7393 - val_iou: 0.6036 - val_dice_loss: 0.2607 - val_jaccard_loss: 0.3948 - val_focal_tversky_loss: 0.4033 - lr: 0.0010\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.0985 - dice_score: 0.9015 - iou: 0.8231 - dice_loss: 0.0985 - jaccard_loss: 0.1743 - focal_tversky_loss: 0.1742\n",
      "Epoch 00022: val_loss did not improve from 0.11443\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "90/90 [==============================] - 28s 309ms/step - loss: 0.0985 - dice_score: 0.9015 - iou: 0.8231 - dice_loss: 0.0985 - jaccard_loss: 0.1743 - focal_tversky_loss: 0.1742 - val_loss: 0.1601 - val_dice_score: 0.8399 - val_iou: 0.7300 - val_dice_loss: 0.1601 - val_jaccard_loss: 0.2680 - val_focal_tversky_loss: 0.2263 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0827 - dice_score: 0.9173 - iou: 0.8480 - dice_loss: 0.0827 - jaccard_loss: 0.1496 - focal_tversky_loss: 0.1541\n",
      "Epoch 00023: val_loss improved from 0.11443 to 0.11104, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 373ms/step - loss: 0.0827 - dice_score: 0.9173 - iou: 0.8480 - dice_loss: 0.0827 - jaccard_loss: 0.1496 - focal_tversky_loss: 0.1541 - val_loss: 0.1110 - val_dice_score: 0.8890 - val_iou: 0.8048 - val_dice_loss: 0.1110 - val_jaccard_loss: 0.1931 - val_focal_tversky_loss: 0.2067 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0773 - dice_score: 0.9227 - iou: 0.8574 - dice_loss: 0.0773 - jaccard_loss: 0.1400 - focal_tversky_loss: 0.1488\n",
      "Epoch 00024: val_loss improved from 0.11104 to 0.10302, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 368ms/step - loss: 0.0773 - dice_score: 0.9227 - iou: 0.8574 - dice_loss: 0.0773 - jaccard_loss: 0.1400 - focal_tversky_loss: 0.1488 - val_loss: 0.1030 - val_dice_score: 0.8970 - val_iou: 0.8195 - val_dice_loss: 0.1030 - val_jaccard_loss: 0.1782 - val_focal_tversky_loss: 0.1865 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0761 - dice_score: 0.9239 - iou: 0.8593 - dice_loss: 0.0761 - jaccard_loss: 0.1382 - focal_tversky_loss: 0.1465\n",
      "Epoch 00025: val_loss did not improve from 0.10302\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.0761 - dice_score: 0.9239 - iou: 0.8593 - dice_loss: 0.0761 - jaccard_loss: 0.1382 - focal_tversky_loss: 0.1465 - val_loss: 0.1076 - val_dice_score: 0.8924 - val_iou: 0.8167 - val_dice_loss: 0.1076 - val_jaccard_loss: 0.1810 - val_focal_tversky_loss: 0.1838 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0736 - dice_score: 0.9264 - iou: 0.8634 - dice_loss: 0.0736 - jaccard_loss: 0.1340 - focal_tversky_loss: 0.1423\n",
      "Epoch 00026: val_loss did not improve from 0.10302\n",
      "90/90 [==============================] - 27s 305ms/step - loss: 0.0736 - dice_score: 0.9264 - iou: 0.8634 - dice_loss: 0.0736 - jaccard_loss: 0.1340 - focal_tversky_loss: 0.1423 - val_loss: 0.1165 - val_dice_score: 0.8835 - val_iou: 0.8054 - val_dice_loss: 0.1165 - val_jaccard_loss: 0.1923 - val_focal_tversky_loss: 0.1873 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0741 - dice_score: 0.9259 - iou: 0.8630 - dice_loss: 0.0741 - jaccard_loss: 0.1345 - focal_tversky_loss: 0.1429\n",
      "Epoch 00027: val_loss improved from 0.10302 to 0.10151, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 374ms/step - loss: 0.0741 - dice_score: 0.9259 - iou: 0.8630 - dice_loss: 0.0741 - jaccard_loss: 0.1345 - focal_tversky_loss: 0.1429 - val_loss: 0.1015 - val_dice_score: 0.8985 - val_iou: 0.8202 - val_dice_loss: 0.1015 - val_jaccard_loss: 0.1775 - val_focal_tversky_loss: 0.1807 - lr: 1.0000e-04\n",
      "Epoch 28/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0735 - dice_score: 0.9265 - iou: 0.8639 - dice_loss: 0.0735 - jaccard_loss: 0.1335 - focal_tversky_loss: 0.1417\n",
      "Epoch 00028: val_loss improved from 0.10151 to 0.09968, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 371ms/step - loss: 0.0735 - dice_score: 0.9265 - iou: 0.8639 - dice_loss: 0.0735 - jaccard_loss: 0.1335 - focal_tversky_loss: 0.1417 - val_loss: 0.0997 - val_dice_score: 0.9003 - val_iou: 0.8268 - val_dice_loss: 0.0997 - val_jaccard_loss: 0.1708 - val_focal_tversky_loss: 0.1765 - lr: 1.0000e-04\n",
      "Epoch 29/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0743 - dice_score: 0.9257 - iou: 0.8626 - dice_loss: 0.0743 - jaccard_loss: 0.1349 - focal_tversky_loss: 0.1430\n",
      "Epoch 00029: val_loss did not improve from 0.09968\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.0743 - dice_score: 0.9257 - iou: 0.8626 - dice_loss: 0.0743 - jaccard_loss: 0.1349 - focal_tversky_loss: 0.1430 - val_loss: 0.1084 - val_dice_score: 0.8916 - val_iou: 0.8172 - val_dice_loss: 0.1084 - val_jaccard_loss: 0.1805 - val_focal_tversky_loss: 0.1868 - lr: 1.0000e-04\n",
      "Epoch 30/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0712 - dice_score: 0.9288 - iou: 0.8679 - dice_loss: 0.0712 - jaccard_loss: 0.1295 - focal_tversky_loss: 0.1375\n",
      "Epoch 00030: val_loss did not improve from 0.09968\n",
      "90/90 [==============================] - 27s 300ms/step - loss: 0.0712 - dice_score: 0.9288 - iou: 0.8679 - dice_loss: 0.0712 - jaccard_loss: 0.1295 - focal_tversky_loss: 0.1375 - val_loss: 0.1085 - val_dice_score: 0.8915 - val_iou: 0.8177 - val_dice_loss: 0.1085 - val_jaccard_loss: 0.1800 - val_focal_tversky_loss: 0.1828 - lr: 1.0000e-04\n",
      "Epoch 31/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0715 - dice_score: 0.9285 - iou: 0.8673 - dice_loss: 0.0715 - jaccard_loss: 0.1302 - focal_tversky_loss: 0.1366\n",
      "Epoch 00031: val_loss did not improve from 0.09968\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.0715 - dice_score: 0.9285 - iou: 0.8673 - dice_loss: 0.0715 - jaccard_loss: 0.1302 - focal_tversky_loss: 0.1366 - val_loss: 0.0999 - val_dice_score: 0.9001 - val_iou: 0.8226 - val_dice_loss: 0.0999 - val_jaccard_loss: 0.1752 - val_focal_tversky_loss: 0.1795 - lr: 1.0000e-04\n",
      "Epoch 32/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0699 - dice_score: 0.9301 - iou: 0.8701 - dice_loss: 0.0699 - jaccard_loss: 0.1275 - focal_tversky_loss: 0.1379\n",
      "Epoch 00032: val_loss improved from 0.09968 to 0.09610, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 34s 374ms/step - loss: 0.0699 - dice_score: 0.9301 - iou: 0.8701 - dice_loss: 0.0699 - jaccard_loss: 0.1275 - focal_tversky_loss: 0.1379 - val_loss: 0.0961 - val_dice_score: 0.9039 - val_iou: 0.8294 - val_dice_loss: 0.0961 - val_jaccard_loss: 0.1684 - val_focal_tversky_loss: 0.1736 - lr: 1.0000e-04\n",
      "Epoch 33/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0714 - dice_score: 0.9286 - iou: 0.8674 - dice_loss: 0.0714 - jaccard_loss: 0.1303 - focal_tversky_loss: 0.1402\n",
      "Epoch 00033: val_loss did not improve from 0.09610\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0714 - dice_score: 0.9286 - iou: 0.8674 - dice_loss: 0.0714 - jaccard_loss: 0.1303 - focal_tversky_loss: 0.1402 - val_loss: 0.1062 - val_dice_score: 0.8938 - val_iou: 0.8213 - val_dice_loss: 0.1062 - val_jaccard_loss: 0.1765 - val_focal_tversky_loss: 0.1753 - lr: 1.0000e-04\n",
      "Epoch 34/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0715 - dice_score: 0.9285 - iou: 0.8675 - dice_loss: 0.0715 - jaccard_loss: 0.1301 - focal_tversky_loss: 0.1375\n",
      "Epoch 00034: val_loss improved from 0.09610 to 0.09205, saving model to saved_models/ISIC_2016/unet_res50/usual_2/exp_2/model\n",
      "90/90 [==============================] - 33s 372ms/step - loss: 0.0715 - dice_score: 0.9285 - iou: 0.8675 - dice_loss: 0.0715 - jaccard_loss: 0.1301 - focal_tversky_loss: 0.1375 - val_loss: 0.0920 - val_dice_score: 0.9080 - val_iou: 0.8354 - val_dice_loss: 0.0920 - val_jaccard_loss: 0.1623 - val_focal_tversky_loss: 0.1630 - lr: 1.0000e-04\n",
      "Epoch 35/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0686 - dice_score: 0.9314 - iou: 0.8724 - dice_loss: 0.0686 - jaccard_loss: 0.1253 - focal_tversky_loss: 0.1346\n",
      "Epoch 00035: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0686 - dice_score: 0.9314 - iou: 0.8724 - dice_loss: 0.0686 - jaccard_loss: 0.1253 - focal_tversky_loss: 0.1346 - val_loss: 0.1098 - val_dice_score: 0.8902 - val_iou: 0.8148 - val_dice_loss: 0.1098 - val_jaccard_loss: 0.1831 - val_focal_tversky_loss: 0.1915 - lr: 1.0000e-04\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.0670 - dice_score: 0.9330 - iou: 0.8751 - dice_loss: 0.0670 - jaccard_loss: 0.1226 - focal_tversky_loss: 0.1317\n",
      "Epoch 00036: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.0670 - dice_score: 0.9330 - iou: 0.8751 - dice_loss: 0.0670 - jaccard_loss: 0.1226 - focal_tversky_loss: 0.1317 - val_loss: 0.1036 - val_dice_score: 0.8964 - val_iou: 0.8222 - val_dice_loss: 0.1036 - val_jaccard_loss: 0.1756 - val_focal_tversky_loss: 0.1835 - lr: 1.0000e-04\n",
      "Epoch 37/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0692 - dice_score: 0.9308 - iou: 0.8714 - dice_loss: 0.0692 - jaccard_loss: 0.1263 - focal_tversky_loss: 0.1355\n",
      "Epoch 00037: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.0692 - dice_score: 0.9308 - iou: 0.8714 - dice_loss: 0.0692 - jaccard_loss: 0.1263 - focal_tversky_loss: 0.1355 - val_loss: 0.0987 - val_dice_score: 0.9013 - val_iou: 0.8291 - val_dice_loss: 0.0987 - val_jaccard_loss: 0.1687 - val_focal_tversky_loss: 0.1788 - lr: 1.0000e-04\n",
      "Epoch 38/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0641 - dice_score: 0.9359 - iou: 0.8800 - dice_loss: 0.0641 - jaccard_loss: 0.1176 - focal_tversky_loss: 0.1272\n",
      "Epoch 00038: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0641 - dice_score: 0.9359 - iou: 0.8800 - dice_loss: 0.0641 - jaccard_loss: 0.1176 - focal_tversky_loss: 0.1272 - val_loss: 0.1027 - val_dice_score: 0.8973 - val_iou: 0.8224 - val_dice_loss: 0.1027 - val_jaccard_loss: 0.1755 - val_focal_tversky_loss: 0.1801 - lr: 1.0000e-04\n",
      "Epoch 39/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0694 - dice_score: 0.9306 - iou: 0.8710 - dice_loss: 0.0694 - jaccard_loss: 0.1266 - focal_tversky_loss: 0.1367\n",
      "Epoch 00039: val_loss did not improve from 0.09205\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "90/90 [==============================] - 27s 301ms/step - loss: 0.0694 - dice_score: 0.9306 - iou: 0.8710 - dice_loss: 0.0694 - jaccard_loss: 0.1266 - focal_tversky_loss: 0.1367 - val_loss: 0.1066 - val_dice_score: 0.8934 - val_iou: 0.8224 - val_dice_loss: 0.1066 - val_jaccard_loss: 0.1754 - val_focal_tversky_loss: 0.1783 - lr: 1.0000e-04\n",
      "Epoch 40/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0668 - dice_score: 0.9332 - iou: 0.8756 - dice_loss: 0.0668 - jaccard_loss: 0.1221 - focal_tversky_loss: 0.1271\n",
      "Epoch 00040: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0668 - dice_score: 0.9332 - iou: 0.8756 - dice_loss: 0.0668 - jaccard_loss: 0.1221 - focal_tversky_loss: 0.1271 - val_loss: 0.0976 - val_dice_score: 0.9024 - val_iou: 0.8305 - val_dice_loss: 0.0976 - val_jaccard_loss: 0.1674 - val_focal_tversky_loss: 0.1715 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0673 - dice_score: 0.9327 - iou: 0.8746 - dice_loss: 0.0673 - jaccard_loss: 0.1231 - focal_tversky_loss: 0.1311\n",
      "Epoch 00041: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0673 - dice_score: 0.9327 - iou: 0.8746 - dice_loss: 0.0673 - jaccard_loss: 0.1231 - focal_tversky_loss: 0.1311 - val_loss: 0.0983 - val_dice_score: 0.9017 - val_iou: 0.8308 - val_dice_loss: 0.0983 - val_jaccard_loss: 0.1671 - val_focal_tversky_loss: 0.1733 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0666 - dice_score: 0.9334 - iou: 0.8757 - dice_loss: 0.0666 - jaccard_loss: 0.1219 - focal_tversky_loss: 0.1323\n",
      "Epoch 00042: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 300ms/step - loss: 0.0666 - dice_score: 0.9334 - iou: 0.8757 - dice_loss: 0.0666 - jaccard_loss: 0.1219 - focal_tversky_loss: 0.1323 - val_loss: 0.0978 - val_dice_score: 0.9022 - val_iou: 0.8291 - val_dice_loss: 0.0978 - val_jaccard_loss: 0.1689 - val_focal_tversky_loss: 0.1760 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0661 - dice_score: 0.9339 - iou: 0.8768 - dice_loss: 0.0661 - jaccard_loss: 0.1209 - focal_tversky_loss: 0.1308\n",
      "Epoch 00043: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0661 - dice_score: 0.9339 - iou: 0.8768 - dice_loss: 0.0661 - jaccard_loss: 0.1209 - focal_tversky_loss: 0.1308 - val_loss: 0.1055 - val_dice_score: 0.8945 - val_iou: 0.8229 - val_dice_loss: 0.1055 - val_jaccard_loss: 0.1750 - val_focal_tversky_loss: 0.1818 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0658 - dice_score: 0.9342 - iou: 0.8771 - dice_loss: 0.0658 - jaccard_loss: 0.1206 - focal_tversky_loss: 0.1311\n",
      "Epoch 00044: val_loss did not improve from 0.09205\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0658 - dice_score: 0.9342 - iou: 0.8771 - dice_loss: 0.0658 - jaccard_loss: 0.1206 - focal_tversky_loss: 0.1311 - val_loss: 0.1148 - val_dice_score: 0.8852 - val_iou: 0.8134 - val_dice_loss: 0.1148 - val_jaccard_loss: 0.1845 - val_focal_tversky_loss: 0.1943 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0666 - dice_score: 0.9334 - iou: 0.8758 - dice_loss: 0.0666 - jaccard_loss: 0.1218 - focal_tversky_loss: 0.1315\n",
      "Epoch 00045: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 304ms/step - loss: 0.0666 - dice_score: 0.9334 - iou: 0.8758 - dice_loss: 0.0666 - jaccard_loss: 0.1218 - focal_tversky_loss: 0.1315 - val_loss: 0.1020 - val_dice_score: 0.8980 - val_iou: 0.8256 - val_dice_loss: 0.1020 - val_jaccard_loss: 0.1724 - val_focal_tversky_loss: 0.1804 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0657 - dice_score: 0.9343 - iou: 0.8773 - dice_loss: 0.0657 - jaccard_loss: 0.1204 - focal_tversky_loss: 0.1300\n",
      "Epoch 00046: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 299ms/step - loss: 0.0657 - dice_score: 0.9343 - iou: 0.8773 - dice_loss: 0.0657 - jaccard_loss: 0.1204 - focal_tversky_loss: 0.1300 - val_loss: 0.0966 - val_dice_score: 0.9034 - val_iou: 0.8310 - val_dice_loss: 0.0966 - val_jaccard_loss: 0.1669 - val_focal_tversky_loss: 0.1744 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0668 - dice_score: 0.9332 - iou: 0.8756 - dice_loss: 0.0668 - jaccard_loss: 0.1221 - focal_tversky_loss: 0.1328\n",
      "Epoch 00047: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0668 - dice_score: 0.9332 - iou: 0.8756 - dice_loss: 0.0668 - jaccard_loss: 0.1221 - focal_tversky_loss: 0.1328 - val_loss: 0.1051 - val_dice_score: 0.8949 - val_iou: 0.8216 - val_dice_loss: 0.1051 - val_jaccard_loss: 0.1763 - val_focal_tversky_loss: 0.1841 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0652 - dice_score: 0.9348 - iou: 0.8783 - dice_loss: 0.0652 - jaccard_loss: 0.1195 - focal_tversky_loss: 0.1293\n",
      "Epoch 00048: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0652 - dice_score: 0.9348 - iou: 0.8783 - dice_loss: 0.0652 - jaccard_loss: 0.1195 - focal_tversky_loss: 0.1293 - val_loss: 0.0980 - val_dice_score: 0.9020 - val_iou: 0.8317 - val_dice_loss: 0.0980 - val_jaccard_loss: 0.1663 - val_focal_tversky_loss: 0.1735 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0675 - dice_score: 0.9325 - iou: 0.8745 - dice_loss: 0.0675 - jaccard_loss: 0.1232 - focal_tversky_loss: 0.1333\n",
      "Epoch 00049: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 302ms/step - loss: 0.0675 - dice_score: 0.9325 - iou: 0.8745 - dice_loss: 0.0675 - jaccard_loss: 0.1232 - focal_tversky_loss: 0.1333 - val_loss: 0.1042 - val_dice_score: 0.8958 - val_iou: 0.8220 - val_dice_loss: 0.1042 - val_jaccard_loss: 0.1759 - val_focal_tversky_loss: 0.1800 - lr: 1.0000e-05\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.0652 - dice_score: 0.9348 - iou: 0.8781 - dice_loss: 0.0652 - jaccard_loss: 0.1195 - focal_tversky_loss: 0.1293\n",
      "Epoch 00050: val_loss did not improve from 0.09205\n",
      "90/90 [==============================] - 27s 303ms/step - loss: 0.0652 - dice_score: 0.9348 - iou: 0.8781 - dice_loss: 0.0652 - jaccard_loss: 0.1195 - focal_tversky_loss: 0.1293 - val_loss: 0.1103 - val_dice_score: 0.8897 - val_iou: 0.8158 - val_dice_loss: 0.1103 - val_jaccard_loss: 0.1822 - val_focal_tversky_loss: 0.1868 - lr: 1.0000e-05\n",
      "2022-02-26 23:10:18,347 - SKIN-CANCER - INFO - Training is done model is saved in saved_models/ISIC_2016/unet_res50/usual_2/exp_2\n",
      "Evaluation\n",
      "48/48 [==============================] - 8s 174ms/step - loss: 0.1048 - dice_score: 0.8952 - iou: 0.8261 - dice_loss: 0.1048 - jaccard_loss: 0.1718 - focal_tversky_loss: 0.1702\n",
      "2022-02-26 23:10:26,734 - SKIN-CANCER - INFO - Test: Loss= 0.10476084798574448, Dice-Score: 0.8952392935752869, IoU: 0.8260501027107239, Dice_loss: 0.10476084798574448, jaccard_loss: 0.17178653180599213, focal_tversky_loss: 0.17024211585521698\n",
      "1/1 [==============================] - 1s 722ms/step - loss: 0.0791 - dice_score: 0.9209 - iou: 0.8533 - dice_loss: 0.0791 - jaccard_loss: 0.1428 - focal_tversky_loss: 0.1707\n",
      "2022-02-26 23:10:35,353 - SKIN-CANCER - INFO - Successfully Saved figures!\n",
      "\n",
      "-----------------------------train 2 is done! ----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!PYTHONHASHSEED=0\n",
    "!TF_DETERMINISTIC_OPS=0\n",
    "!TF_CUDNN_DETERMINISTIC=0\n",
    "for n in range(multi_train):\n",
    "    seed = seeds + n\n",
    "    saved_path_name = f\"{train_identifier}_{n}\"\n",
    "    !python train.py \\\n",
    "    --model $model --dataset_name $dataset_name \\\n",
    "    --save_path $save_path \\\n",
    "    --save_path_name $saved_path_name \\\n",
    "    --epochs $epochs \\\n",
    "    --optimizer $optimizer \\\n",
    "    --lr $lr \\\n",
    "    --min_lr $min_lr \\\n",
    "    --reduce_lr_patience $reduce_lr_patience \\\n",
    "    --reduce_lr_factor $reduce_lr_factor \\\n",
    "    --early_stopping_p $early_stopping_p \\\n",
    "    --hair_aug_p $hair_aug_p \\\n",
    "    --hair_rmv_p $hair_rmv_p \\\n",
    "    --random_rotate_p $random_rotate_p \\\n",
    "    --p_horizontal_flip $p_horizontal_flip \\\n",
    "    --p_vertical_flip $p_vertical_flip \\\n",
    "    --p_center_crop $p_center_crop \\\n",
    "    --mosaic_p $mosaic_p \\\n",
    "    --cutmix_p $cutmix_p \\\n",
    "    --cutmix_beta $cutmix_beta \\\n",
    "    --seed $seed\n",
    "    \n",
    "    print(f\"\\n-----------------------------train {n} is done! ----------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977a1b7",
   "metadata": {
    "id": "0977a1b7"
   },
   "source": [
    "# Get Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34cf3098",
   "metadata": {
    "id": "34cf3098",
    "outputId": "d581a2c5-f802-4e46-ebdb-8f9220da1b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting values from csv files...\n",
      "[INFO] Getting the values of file saved_models/ISIC_2016/unet_res50/usual_0/exp_2/csv_logger_train.csv\n",
      "[INFO] Getting the values of file saved_models/ISIC_2016/unet_res50/usual_1/exp_2/csv_logger_train.csv\n",
      "[INFO] Getting the values of file saved_models/ISIC_2016/unet_res50/usual_2/exp_2/csv_logger_train.csv\n",
      "{'dice_loss': {'std': 0.0041, 'mean': 0.0677}, 'dice_score': {'std': 0.0041, 'mean': 0.9323}, 'focal_tversky_loss': {'std': 0.0062, 'mean': 0.1324}, 'iou': {'std': 0.007, 'mean': 0.8739}, 'jaccard_loss': {'std': 0.0063, 'mean': 0.1225}, 'loss': {'std': 0.0041, 'mean': 0.0677}, 'val_dice_loss': {'std': 0.0055, 'mean': 0.0904}, 'val_dice_score': {'std': 0.0055, 'mean': 0.9096}, 'val_focal_tversky_loss': {'std': 0.0067, 'mean': 0.1612}, 'val_iou': {'std': 0.0085, 'mean': 0.8366}, 'val_jaccard_loss': {'std': 0.0088, 'mean': 0.1597}, 'val_loss': {'std': 0.0055, 'mean': 0.0904}}\n"
     ]
    }
   ],
   "source": [
    "from utils.group_metrics import get_mean_std\n",
    "import os\n",
    "\n",
    "csv_addresses = [\n",
    " os.path.join(save_path, dataset_name, model, f\"{train_identifier}_{n}\", \"exp_2\" , \"csv_logger_train.csv\") for n in range(multi_train)   \n",
    "]\n",
    "metrics = get_mean_std(csv_addresses, \n",
    "                       arguments=(\n",
    "                           \"dice_loss\",\n",
    "                           \"dice_score\",\n",
    "                           \"focal_tversky_loss\",\n",
    "                           \"iou\",\n",
    "                           \"jaccard_loss\",\n",
    "                           \"loss\",\n",
    "                           \"val_dice_loss\",\n",
    "                           \"val_dice_score\",\n",
    "                           \"val_focal_tversky_loss\",\n",
    "                           \"val_iou\",\n",
    "                           \"val_jaccard_loss\",\n",
    "                           \"val_loss\"\n",
    "                       ),\n",
    "                       operators=(min, max, min, max, min, min, min, max, min, max, min, min)\n",
    "                      )\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "410c201e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ISIC_2016/unet_res50/usual_0/exp_2/csv_logger_train.csv',\n",
       " 'ISIC_2016/unet_res50/usual_1/exp_2/csv_logger_train.csv',\n",
       " 'ISIC_2016/unet_res50/usual_2/exp_2/csv_logger_train.csv']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49feee3c",
   "metadata": {},
   "source": [
    "_:)_"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
